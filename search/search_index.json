{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nesta Data Science Cookiecutter","text":"<p>A standard project structure for reproducible and collaborative data science projects @ Nesta.</p>"},{"location":"#high-level-aims","title":"High-level aims","text":"<ul> <li>Get going on a new project quickly (but not too quickly!)</li> <li>Nudge users to using a common structure and best-practices to:<ul> <li>Enable data scientists @ Nesta to work with each other</li> <li>Increase reliability of data science @ Nesta</li> <li>Make our projects more reproducible</li> <li>Increase the value of codebases, and accompanying documentation/reports to stakeholders</li> <li>Make code easier to understand</li> </ul> </li> <li>Conceptual/structural overlap with production systems allows data scientists to get code into production faster/easier</li> <li>Teams/projects empowered to build on foundation</li> </ul>"},{"location":"developers/","title":"Developers guide","text":""},{"location":"developers/#installation","title":"Installation","text":"<p>This project uses <code>uv</code> to manage the virtual environment. To install the project, run:</p> <pre><code>uv sync\nuv run pre-commit install --install-hooks\n</code></pre> <p>This will create a virtual environment in <code>.venv</code> and install the dependencies listed in <code>pyproject.toml</code>.</p> <p>You can then go on to build docs, run tests, etc. using <code>uv run</code> commands.</p>"},{"location":"developers/#documentation","title":"Documentation","text":"<p>We use mkdocs and mkdocs material to maintain documentation. You can test them locally with:</p> <pre><code>uv run mkdocs serve\n</code></pre> <p>:note: mkdocs material uses a superset of Markdown, see the reference</p> <p>Docs are automatically published to <code>gh-pages</code> branch via. Github actions after a PR is merged into <code>master</code>.</p>"},{"location":"developers/#testing","title":"Testing","text":"<p>We use pytest to run tests. You can run them with:</p> <pre><code>uv run pytest\n</code></pre> <p>You can also run tests with coverage using:</p> <pre><code>uv run pytest --cov=.\n</code></pre>"},{"location":"developers/#github-actions","title":"Github actions","text":"<p>There are several workflows in <code>.github/workflows</code>:</p> <ul> <li><code>docs.yml</code> - Deploys documentation to github pages on push to master (e.g. after PR merged)</li> <li><code>release.yml</code> - Drafts release notes based on merged PR's</li> <li><code>labeler.yml</code> - Sets the repository to use the issue labels defined in <code>.github/labels.yml</code></li> <li><code>test.yml</code> - Runs tests on PR's or on push to master (e.g. after PR merged)</li> </ul>"},{"location":"developers/#release-process","title":"Release process","text":"<ul> <li>Each PR should have a <code>major</code>/<code>minor</code>/<code>patch</code> label assigned based on the desired version increment, e.g. <code>minor</code> will go from <code>x.y.z -&gt; x.(y+1).z</code></li> <li>After a PR is merged then draft release notes will be generated/updated here (see <code>release.yml</code> above)</li> <li>In the Github UI: rewrite the drafts into something informative to a user and then click release<ul> <li>:warning: Releases should be made little and often - commits on <code>master</code> are immediately visible to cookiecutter users</li> </ul> </li> </ul>"},{"location":"quickstart/","title":"Project Set Up","text":""},{"location":"quickstart/#your-guide-to-setting-up-a-nesta-cookiecutter-project","title":"Your Guide to Setting Up a Nesta Cookiecutter Project","text":"<p>In this page you will learn how to set up a project using the cookiecutter. The steps are different depending on whether you are the first one setting up a project (Project Configurer) or whether a project already exists and you are just setting it up locally (Team Member).</p>"},{"location":"quickstart/#project-configuration","title":"Project Configuration","text":"<p>Prerequisite: If this is your first time setting up a cookiecutter project you need to install it from https://pypi.org/project/cookiecutter. You can do this in the terminal with <code>brew install cookiecutter</code>.</p>"},{"location":"quickstart/#1-request-repository-setup","title":"1. Request repository setup","text":"<p>First things first, you need a repo created for you. From the tech support website, go to Ask Nesta Technology, Request Forms, then Request GitHub repository. You will need to provide a project name, repo name, whether public/private, github teams involved, team leading the project, short and long description of the project. An empty repo will automatically be set up for you within a few minutes: https://github.com/nestauk/your_repo_name. Your team should have admin access on the repository.</p>"},{"location":"quickstart/#2-set-up-your-project-locally","title":"2. Set up your project locally","text":"<p>It is important that you do not clone the repo yet! Instead, open the terminal and <code>cd</code> to a folder where you eventually want your repo to be, then run the cookiecutter:</p> <pre><code>cookiecutter https://github.com/nestauk/ds-cookiecutter\n</code></pre> <p>This will automatically install the latest version. If you want to install a different version run:</p> <pre><code>cookiecutter https://github.com/nestauk/ds-cookiecutter -c &lt;VERSION TAG&gt;\n</code></pre> <p>You will be prompted to enter the following information:</p> <ul> <li><code>You've downloaded ~.cookiecutters/ds-cookiecutter before. Is it okay to delete and re-download it?[yes]</code> press Enter to confirm yes, it's always best to use the latest version.</li> <li><code>project_name [project_name]</code>: Enter the title of your project. This will be used in the <code>README.md</code> file and docs.</li> <li><code>module_name [project_name]</code>: This defaults to a sanitised (lower-case, no spaces, no numbers) version of the project name (used in <code>pyproject.toml</code> and throughout).</li> </ul> <ul> <li><code>description [A short description of the project.]</code>: Add a short description</li> <li><code>openness [public]</code>: This determines the licence, this can be changed in the future if needed</li> <li><code>python_version [3.13]</code>: the behaviour of this prompt depends on the <code>venv_type</code> you selected. If you selected <code>uv</code> you may provide a PEP compliant expression for the <code>pyproject.toml</code> (e.g. <code>&gt;=3.10</code>, <code>==3.11.*</code>, etc.). If you selected <code>venv</code> it must be a single version available on your system (e.g. <code>3.10</code>, <code>3.11</code>, etc.). If you selected <code>conda</code>, you needn't have the version installed but again you must specify a single version (e.g. <code>3.10</code>, <code>3.10.2</code>, etc.). The <code>pyproject.toml</code> will be created with the specified version.</li> <li><code>venv_type [uv]</code>: choose how you will manage your virtual environment, the options are <code>uv</code>, <code>venv</code> or <code>conda</code>.</li> <li><code>file_structure [standard]</code>: choose the complexity of your project. The options are: <code>simple</code> (basic and recommended for small projects); <code>standard</code> (a good balance between simplicity and complexity); and <code>full</code> (includes folders for documentation and testing).</li> <li><code>autosetup [yes]</code>: this will automatically set up the project's virtual environment, pre-commit hooks and git repository for you. If you select <code>no</code>, you will have to do this manually later.'</li> </ul>"},{"location":"quickstart/#3-connect-your-local-project-to-github","title":"3. Connect your local project to github","text":"<p>You have set up your project locally and now you have to connect it to the remote repo. When you change directory to your created project folder, you will see that you are in a git repository and the generated cookiecutter has committed itself to the <code>main</code> and <code>dev</code> branches. Connect to the git repo by running <code>git remote add origin git@github.com:nestauk/&lt;REPONAME&gt;</code> to point your local project to the configured repository.</p> <p>Now you are all set!</p>"},{"location":"quickstart/#team-members","title":"Team Members","text":"<p>Clone the repository and <code>cd</code> into it; you can then ascertain the <code>venv_type</code> used in the project's creation. This can be inferred by the presence of a <code>uv.lock</code> file for <code>uv</code> or an <code>environment.yaml</code> file for <code>conda</code>.</p> <p>The author should then have included a <code>README.md</code> file with instructions on how to set up the project. If not, consult the documentation for the <code>venv_type</code> or ask the author for help.</p>"},{"location":"structure/","title":"Structure","text":"<p>This page gives a guide to where things belong within the cookiecutter structure.</p> <p>A direct tree representation of the folder hierarchy is also given at the bottom.</p> <p>Here are a couple of examples from projects:</p> <ul> <li> <p>AHL - Out of Home Analysis</p> </li> <li> <p>AFS - Birmingham Early Years Data</p> </li> <li> <p>ASF - Heat Pump Readiness</p> </li> <li> <p>DS - Green Jobs</p> </li> </ul> <p>Note: In the following sections we use <code>src/</code> to denote the project name to avoid awkward <code>&lt;project_name&gt;</code> placeholders.</p>"},{"location":"structure/#project-configuration","title":"Project configuration","text":"<p>Depending on your choice of <code>autosetup</code> the cookiecutter will either stop at establishing the project structure, or it will do the following for you:</p> <ol> <li>Set up your virtual environment using your selected tool, such as <code>uv</code>, <code>poetry</code>, or <code>conda</code>.</li> <li>Set up pre-commit and install the hooks specified in <code>.pre-commit-config.yaml</code>.</li> <li>Run <code>direnv allow</code> in context of the project so that the <code>.env</code> and <code>.envrc</code> files are automatically loaded (importing environment variables and activating the relevant virtual environment).</li> <li>Set up the <code>git</code> repository using <code>git init</code> and create the initial commit on <code>main</code> and <code>dev</code> branches.</li> <li>Force push the initial commit to the remote repository (if specified) using <code>git push -u origin main</code> and <code>git push -u origin dev</code>.</li> </ol> <p>These steps can be completed manually if the <code>autosetup</code> option was not used. Critically, they (with the exception of the <code>git</code> parts when cloning a repo) are the first steps we should take when working in any repository, whether it's a new project or a clone of a colleague's!</p> <p>**The following sections are potentially outdated but go into greater detail about the steps above. **</p>"},{"location":"structure/#git-hooks","title":"Git hooks","text":"<p>We use pre-commit to check the integrity of git commits before they happen. You should set this up with your virtual environment. With <code>uv</code>, it's as simple as adding it to the environment, sourcing the environment, and running <code>pre-commit install --install-hooks</code>.</p> <pre><code>uv add pre-commit\nsource .venv/bin/activate\npre-commit install --install-hooks\n</code></pre> <p>The steps are specified in <code>.pre-commit-config.yaml</code>. Some basic checks are performed on the repository (ensuring no large files commited and fixing trailing whitespace) from a core set of pre-commit hooks. More critically, we use ruff to format and lint the repository, an all-in-one alternative to black, flake8, and isort.</p> <p>Now, every time you run <code>git commit</code>, it should perform these checks automatically</p> <p>Warning: You need to run <code>git commit</code> with your virtual environment activated. This is because by default the packages used by pre-commit are installed into your project's environment. (note: <code>pre-commit install --install-hooks</code> will install the pre-commit hooks in the currently active environment, which is why we source it before running in the example above).</p>"},{"location":"structure/#reproducible-environment","title":"Reproducible environment","text":"<p>The first step in reproducing someone else\u2019s analysis is to reproduce the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together.</p> <p>By listing all of your requirements in the repository you can easily track the packages needed to recreate the analysis. We recommend the use of uv to manage the requirements in your project, but you are open to choose other tools such as poetry, Python's built in venv, or conda.</p> <p>Whilst popular for scientific computing and data science, conda poses problems for collaboration and packaging, so we recommend moving away from its use if at all possible.</p>"},{"location":"structure/#files","title":"Files","text":"<p>The files used to track dependencies in a virtual environment will vary based on the tool you use. For <code>uv</code> and <code>poetry</code>, the entire project will be tracked using the <code>pyproject.toml</code> file. The <code>pyproject.toml</code> file has already been pre-filled in the cookiecutter, but you should learn how it works so you can make edits and changes as necessary within your projects.</p> <p>You can add or remove dependencies directly from the command line, using <code>uv add pandas</code> or <code>uv remove numpy</code> to automatically edit the <code>pyproject.toml</code> file. You can add development dependencies with the <code>--dev</code> flag like so <code>uv add --dev ipykernel</code>. You can substitute <code>poetry</code> for <code>uv</code> depending on the tool you're using.</p> <p>If using <code>conda</code> or Python's built-in environment manager, you may manually manage requirements in text files, like <code>requirements.txt</code> and <code>requirements_dev.txt</code>. <code>conda</code> will also use an <code>environment.yaml</code> file to define the base conda environment and any dependencies not \"pip-installable\".</p>"},{"location":"structure/#commands","title":"Commands","text":"<p>You will need to create your virtual environment, install the packages, and activate it using the tool you've selected. Previously, <code>make install</code> helped with this setup and updating. Using <code>uv</code>, you can run the following:</p> <ol> <li><code>uv venv</code> to create a virtual environment in the <code>.venv</code> folder</li> <li><code>uv add package</code> to add dependencies</li> <li><code>uv sync</code> to ensure your virtual environment is synced with the <code>uv</code> lockfile, especially useful when entering new projects</li> <li><code>uv pip install -e .</code> to install the project to your environment in an editable format</li> <li><code>source .venv/bin/activate</code> to activate the virtual environment, or <code>uv run script.py</code> to directly run any Python script in the environment</li> </ol> <p>These are just some of the commands you might want to run, if you chose <code>uv</code>. You should learn and explore whatever tool you choose so you can confidently manage your projects and its dependencies.</p>"},{"location":"structure/#secrets-and-configuration-env-and-srcconfig","title":"Secrets and configuration - <code>.env.*</code> and <code>src/config/*</code>","text":"<p>You really don't want to leak your AWS secret key or database username and password on Github. To avoid this you can:</p>"},{"location":"structure/#store-your-secrets-in-a-special-file","title":"Store your secrets in a special file","text":"<p>Create a <code>.env</code> file in the project root folder. Thanks to the <code>.gitignore</code>, this file should never get committed into the version control repository.</p> <p>Here's an example:</p> <pre><code># example .env file\nDATABASE_URL=postgres://username:password@localhost:5432/dbname\nOTHER_VARIABLE=something\n</code></pre> <p>We also have <code>.envrc</code> which contains non-secret project configuration shared across users such as the bucket that our input data is stored in.</p> <p><code>direnv</code> automatically loads <code>.envrc</code> (which itself loads <code>.env</code>) making our configuration available. Add all environment variables directly to <code>.env</code> so they are available in Python through <code>dotenv</code> as well as in your terminal through <code>direnv</code>. You will need to activate <code>direnv</code> in the repository by running <code>direnv allow</code>.</p>"},{"location":"structure/#store-data-science-configuration-in-srcconfig","title":"Store Data science configuration in <code>src/config/</code>","text":"<p>If there are certain variables that are useful throughout a codebase, it is useful to store these in a single place rather than having to define them throughout the project.</p> <p><code>src/config/base.yaml</code> provides a place to document these global variables.</p> <p>For example, if you were working on a fuzzy-matching the PATSTAT patent database to the Companies House database and wanted to only merge above a certain match score you may add a section to the configuration like the following,</p> <pre><code>patstat_companies_house:\n    match_threshold: 90\n</code></pre> <p>and load that value into your code with,</p> <pre><code>from src import config\n\nconfig[\"patstat_companies_house\"][\"match_threshold\"]\n</code></pre> <p>This centralisation provides a clearer log of decisions and decreases the chance that a different match threshold gets incorrectly used somewhere else in the codebase.</p> <p>Config files are also useful for storing model parameters. Storing model parameters in a config makes it much easier to test different model configurations and document and reproduce your model once it\u2019s been trained. You can easily reference your config file to make changes and write your final documentation rather than having to dig through code. Depending on the complexity of your repository, it may make sense to create separate config files for each of your models.</p> <p>For example, if training an SVM classifier you may want to test different values of the regularisation parameter \u2018C\u2019. You could create a file called <code>src/config/svm_classifier.yaml</code> to store the parameter values in the same way as before.</p> <p>Note - as well as avoiding hard-coding parameters into our code, we should never hard-code full file paths, e.g. <code>/home/Projects/my_fantastic_data_project/outputs/data/foo.json</code>, as this will never work on anything other than your machine.</p> <p>Instead use relative paths and make use of <code>src.PROJECT_DIR</code> which will return the path to your project's base directory. This means you could specify the above path as <code>f\"{src.PROJECT_DIR}/outputs/data/foo.json\"</code> and have it work on everyone's machine!</p>"},{"location":"structure/#data-inputsdata-outputsdata-outputscache","title":"Data - <code>inputs/data</code>, <code>outputs/data</code>, <code>outputs/.cache</code>","text":"<p>Generally, don't version control data (inputs or outputs) in git, it is best to use s3 (directly or through metaflow) to manage your data.</p>"},{"location":"structure/#inputsdata","title":"<code>inputs/data</code>","text":"<p>Put any data dependencies of your project that your code doesn't fetch here (E.g. if someone emailed you a spreadsheet with the results of a randomised control trial).</p> <p>Don't ever edit this raw data, especially not manually or in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable.</p> <p>Ideally, you should store it in AWS S3. You can then use the ds-utils package, which has a neat way of pulling in data into dataframe. Alternatively, if you set the <code>S3_INPUT_PATH</code> environment variable (e.g. in <code>.env</code>) then you can use <code>make inputs-pull</code> to pull data from the configured S3 bucket.</p>"},{"location":"structure/#outputscache","title":"<code>outputs/.cache/</code>","text":"<p>This folder is for ephemeral data and any pipeline/analysis step should be runnable following the deletion of this folder's contents.</p> <p>For example, this folder could be used as a file-level cache (careful about cache invalidation!); to download a file from the web before immediately reading, transforming, and saving as a clean file in <code>outputs/data</code>; or to temporary data when prototyping.</p>"},{"location":"structure/#outputsdata","title":"<code>outputs/data</code>","text":"<p>This folder should contain transformed/processed data that is to be used in the final analysis or is a data output of the final analysis. Again, if this data is sensitive, it is always best to save on S3 instead!</p> <p>Try to order this folder logically. For example, you may want subfolders organised by dataset, sections of analysis, or some other hierarchy that better captures your project.</p>"},{"location":"structure/#fetchingloading-data-srcgetters","title":"Fetching/loading data - <code>src/getters</code>","text":"<p>This folder should contain modules and functions which load our data. Anywhere in the code base we need to load data we should do so by importing and calling a getter (except prototyping in notebooks).</p> <p>This means that lots of calls like <code>pd.read_csv(\"path/to/file\", sep=\"\\t\", ...)</code> throughout the codebase can be avoided.</p> <p>Following this approach means:</p> <ul> <li>If the format of <code>path/to/file</code> changes then we only have to make the change in one place</li> <li>We avoid inconsistencies such as forgetting to read a column in as a <code>str</code> instead of an <code>int</code> and thus missing leading zeros</li> <li>If we want to see what data is available, we have a folder in the project to go to and we let the code speak for itself as much as possible - e.g. the following is a lot more informative than an inline call to <code>pd.read_csv</code> like we had above</li> </ul> <p>Here are two examples:</p> <pre><code>    # File: getters/companies_house.py\n    \"\"\"Data getters for the companies house data.\n\n    Data source: https://download.companieshouse.gov.uk/en_output.html\n    \"\"\"\n    import pandas as pd\n\n    def get_sector() -&gt; pd.DataFrame:\n        \"\"\"Load Companies House sector labels.\n\n        Returns:\n            Sector information for ...\n        \"\"\"\n        return pd.read_csv(\"path/to/file\", sep=\"\\t\", dtype={\"sic_code\": str})\n</code></pre> <p>or using ds-utils:</p> <pre><code>    #File: getters/asq_data.py\n    \"\"\"Data getters for the ASQ data.\n    \"\"\"\n    import pandas as pd\n    from nesta_ds_utils.loading_saving import S3\n\n    def get_asq_data() -&gt; pd.DataFrame:\n    \"\"\"Load ASQ data for assessments taken in 2022.\n\n\n    Returns: Dataframe of the ASQ data at individual level including information on \u2026\n    \"\"\"\n    return S3.download_obj(\n        bucket=\"bucket_name\",\n        path_from=\"data/raw/data_asq.csv\",\n        download_as=\"dataframe\",\n        kwargs_reading={\"engine\": \"python\"},\n    )\n\n</code></pre>"},{"location":"structure/#pipeline-components-srcpipeline","title":"Pipeline components - <code>src/pipeline</code>","text":"<p>This folder contains pipeline components. Put as much data science as possible here.</p> <p>We recommend the use of metaflow to write these pipeline components.</p> <p>Using metaflow:</p> <ul> <li>Gives us lightweight version control of data and models</li> <li>Gives us easy access to AWS batch computing (including GPU machines)</li> <li>Makes it easy to take data-science code into production</li> </ul>"},{"location":"structure/#shared-utilities-srcutils","title":"Shared utilities - <code>src/utils</code>","text":"<p>This is a place to put utility functions needed across different parts of the codebase.</p> <p>For example, this could be functions shared across different pieces of analysis or different pipelines.</p>"},{"location":"structure/#analysis-srcanalysis","title":"Analysis - <code>src/analysis</code>","text":"<p>Functionality in this folder takes the pipeline components (possibly combining them) and generates the plots/statistics to feed into reports.</p> <p>It is easier to say when shomething shouldn't be in <code>analysis</code> than when something should: If one part in <code>analysis</code> depends on another, then that suggests that the thing in common is likely either a pipeline component or a shared utility (i.e. sections of <code>analysis</code> should be completely independent).</p> <p>It is important that plots are saved in <code>outputs/</code> rather than in different areas of the repository.</p>"},{"location":"structure/#notebooks-srcanalysisnotebooks","title":"Notebooks - <code>src/analysis/notebooks</code>","text":"<p>Notebook packages like Quarto are effective tools for exploratory data analysis, fast prototyping, and communicating results; however, between prototyping and communicating results code should be factored out into proper python modules. Compared with Jupyter notebooks, Quarto <code>.qmd</code> files are Markdown based text files that render outputs to separate files, making it easier to code review the raw text and ensure sensitive outputs are not version controlled.</p> <p>We have a notebooks folder for all your notebook needs! For example, if you are prototyping a \"sentence transformer\" you can place the notebooks for prototyping this feature in notebooks, e.g. <code>analysis/notebooks/sentence_transformer/</code> or <code>analysis/notebooks/pipeline/sentence_transformer/</code>.</p> <p>Please try to keep all notebooks within this folder and primarily not on github, especially for code refactoring as the code will be elsewhere, e.g. in the pipeline. However, for collaborating, sharing and QA of analysis, you are welcome to push those to github. Make sure that you do not save rendered files (e.g. HTML) directly to GitHub.</p>"},{"location":"structure/#refactoring","title":"Refactoring","text":"<p>Everybody likes to work differently. Some like to eagerly refactor, keeping as little in notebooks as possible (or even eschewing notebooks entirely); whereas others prefer to keep everything in notebooks until the last minute.</p> <p>You are welcome to work in whatever way you\u2019d like, but try to always submit a pull request (PR) for your feature with everything refactored into python modules.</p> <p>We often find it easiest to refactor frequently, otherwise you might get duplicates of functions across the codebase , e.g. if it's a data preprocessing task, put it in the pipeline at <code>src/pipelines/&lt;descriptive name for task&gt;</code>; if it's useful utility code, refactor it to <code>src/utils/</code>; if it's loading data, refactor it to <code>src/getters</code>.</p>"},{"location":"structure/#tips","title":"Tips","text":"<p>Add the following to your notebook (or IPython REPL):</p> <pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <p>Now when you save code in a python module, the notebook will automatically load in the latest changes without you having to restart the kernel, re-import the module etc.</p>"},{"location":"structure/#share-with-gists","title":"Share with gists","text":"<p>As the git filter above stops the sharing of outputs directly via. the git repository, another way is needed to share the outputs of quick and dirty analysis. We suggest using Gists, particularly the Gist it notebook extension which adds a button to your notebook that will instantly turn the notebook into a gist and upload it to your github (as public/private) as long. This requires <code>jupyter_nbextensions_configurator</code> and a github personal access token.</p>"},{"location":"structure/#dont-install-jupyterjupyterlab-in-your-environment-use-ipykernel","title":"Don't install <code>jupyter</code>/<code>jupyterlab</code> in your environment, use <code>ipykernel</code>","text":"<p>You should avoid <code>jupyter</code>/<code>jupyterlab</code> as a dependency in the project environment.</p> <p>Instead add <code>ipykernel</code> as a dependency. This is a lightweight dependency that allows <code>jupyter</code>/<code>jupyterlab</code> installed elsewhere (e.g. your main conda environment or system installation) to run the code in your project.</p> <p>Run <code>python -m ipykernel install --user --name=&lt;project environment name&gt;</code> from within your project environment to allow jupyter to use your project's virtual environment.</p> <p>The advantages of this are:</p> <ul> <li>You only have to configure <code>jupyter</code>/<code>jupyterlab</code> once</li> <li>You will save disk-space</li> <li>Faster install</li> <li>Colleagues using other editors don't have to install heavy dependencies they don't use</li> </ul> <p>Note: <code>ipykernel</code> is also listed in <code>requirements_dev.txt</code> so you do not need to add it.</p>"},{"location":"structure/#report-outputsreports","title":"Report - <code>outputs/reports</code>","text":"<p>You can write reports in markdown and put them in <code>outputs/reports</code> and reference plots in <code>outputs/figures</code>.</p>"},{"location":"structure/#tree","title":"Tree","text":"<p>Note that this is for the <code>full</code> file structure. The <code>standard</code> and <code>simple</code> file structures are similar but with fewer folders under <code>&lt;MODULE NAME&gt;</code> and the omission of the <code>docs</code> and <code>tests</code> folders.</p> <pre><code>.\n\u251c\u2500\u2500 &lt;MODULE NAME&gt;                    |  PYTHON PACKAGE\n\u2502   \u251c\u2500\u2500 __init__.py                  |\n\u2502   \u251c\u2500\u2500 analysis/                    |  Analysis\n\u2502   \u251c\u2500\u2500 config                       |  Configuration\n\u2502   \u2502   \u251c\u2500\u2500 logging.yaml             |    logging configuration\n\u2502   \u2502   \u251c\u2500\u2500 base.yaml                |    global configuration (e.g. for tracking hyper-parameters)\n\u2502   \u2502   \u2514\u2500\u2500 pipeline/                |    pipeline configuration files\n\u2502   \u251c\u2500\u2500 getters/                     |  Data getters\n\u2502   \u251c\u2500\u2500 notebooks/                   |  Notebooks\n\u2502   \u251c\u2500\u2500 pipeline/                    |  Pipeline components\n\u2502   \u2514\u2500\u2500 utils/                       |  Utilities\n\u251c\u2500\u2500 docs/                            |  DOCUMENTATION\n\u251c\u2500\u2500 pyproject.toml                   |  PROJECT METADATA AND CONFIGURATION\n\u251c\u2500\u2500 environment.yaml                 |  CONDA ENVIRONMENT SPECIFICATION (optional component)\n\u251c\u2500\u2500 LICENSE                          |\n\u251c\u2500\u2500 outputs/                         |  OUTPUTS PRODUCED FROM THE PROJECT\n\u251c\u2500\u2500 README.md                        |\n\u251c\u2500\u2500 .pre-commit-config.yaml          |  DEFINES CHECKS THAT MUST PASS BEFORE git commit SUCCEEDS\n\u251c\u2500\u2500 .gitignore                       |  TELLS git WHAT FILES WE DON'T WANT TO COMMIT\n\u251c\u2500\u2500 .github/                         |  GITHUB CONFIGURATION\n\u251c\u2500\u2500 .env                             |  SECRETS (never commit to git!)\n\u251c\u2500\u2500 .envrc                           |  SHARED PROJECT CONFIGURATION VARIABLES\n\u2514\u2500\u2500 .cookiecutter                    |  COOKIECUTTER SETUP &amp; CONFIGURATION (user can safely ignore)\n</code></pre>"},{"location":"structure/#the-old-makefile","title":"The (old) Makefile","text":"<p>Where did the Makefile go? Previously, our cookiecutter used a Makefile to help manage the environment and its dependencies. However, due to its growing size and complexity, it was becoming difficult to maintain. The Makefile commands were also obfuscating the underlying tools it used, preventing users from growing their experience and confidence in directly managing their project's environments.</p> <p>We decided to do away with the Makefile and hope to support everyone in their journey of directly managing their projects and dependencies directly.</p>"}]}