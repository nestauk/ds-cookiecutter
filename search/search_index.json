{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nesta Data Science Cookiecutter","text":"<p>A standard project structure for reproducible and collaborative data science projects @ Nesta.</p>"},{"location":"#high-level-aims","title":"High-level aims","text":"<ul> <li>Get going on a new project quickly (but not too quickly!)</li> <li>Nudge users to using a common structure and best-practices to:<ul> <li>Enable data scientists @ Nesta to work with each other</li> <li>Increase reliability of data science @ Nesta</li> <li>Make our projects more reproducible</li> <li>Increase the value of codebases, and accompanying documentation/reports to stakeholders</li> <li>Make code easier to understand</li> </ul> </li> <li>Conceptual/structural overlap with production systems allows data scientists to get code into production faster/easier</li> <li>Teams/projects empowered to build on foundation</li> </ul>"},{"location":"examples/","title":"Example project structures","text":"<p>Coming soon.</p>"},{"location":"faq/","title":"FAQ","text":"<p>Please submit questions as a Github issue with the label \"question\".</p>"},{"location":"faq/#what-customisations-can-i-make-when-setting-up-the-cookiecutter","title":"What customisations can I make when setting up the cookiecutter?","text":"<p>You should give thought when modifying the cookiecutter - big breaking changes defeat the point of having a standard starting point.</p> <p>The other side of the equation is having a hackable standard starting point - the cookiecutter aims to be small and simple enough to understand which allows customisation based on needs that can't be centrally anticipated.</p> <p>The <code>.recipes/</code> folder provides scripts/docs for extending the cookiecutter with specific bits of functionality we think are important but are either:</p> <ul> <li>Not suitable in every situation</li> <li>Not generically configurable for every situation</li> <li>Too much complex to be a cookiecutter default</li> </ul> <p>This keeps the core of the cookiecutter lean, maintainable, and hackable.</p> <p>Older versions of the cookiecutter were more opinionated</p> <p>However trying to meet all needs means the cookiecutter ends up too complex and too brittle leaving little room/incentive for sub-teams/projects to understand and extend the cookiecutter to better solve their problems which results in a sub-optimal solution for everyone.</p>"},{"location":"faq/#where-should-i-save-models","title":"Where should I save models?","text":"<ul> <li>If it's a pre-trained model someone has sent you: <code>inputs/models/</code></li> <li>If it's a model you have trained: <code>outputs/models/</code></li> </ul>"},{"location":"guidelines/","title":"Technical and working style guidelines","text":"<p>Challenging this documentation is encouraged. Please set up an issue for challenges, additions or corrections. For explanations, please consult the <code>dev</code> Slack channel</p> <p>In this document we set out some basic \"tips\" (either mandatory or encouraged) which should guide your way of working, regardless of the task.</p> <ul> <li>Foreword</li> <li><code>python is not pandas</code></li> <li>Design patterns</li> <li>Programming</li> <li>Critical thinking</li> <li>Naming conventions</li> <li>Spaces and spacing</li> <li>Comments and docs</li> </ul>"},{"location":"guidelines/#foreword","title":"Foreword","text":"<p>In advance, we recommend installing the autoformatter black in your IDE. In future we will use <code>flake8</code> (or similar) for automatically checking our python codebases.</p> <p>Hopefully the conventions laid out here are the easy and intuitive set of pep8.</p> <p>Code reviewers: it is on you to ensure that this style guide has been followed: there are no points for being lenient, but there [non-redeemable] points for being opinionated! We should all feel pressured into making sure that our code meets an acceptable standard.</p>"},{"location":"guidelines/#python-is-not-pandas","title":"<code>python is not pandas</code>","text":"<p>tldr; Using <code>pandas</code> as a means to perform transformations or calculations on your data should be avoided, unless it clearly simplifies the logic and readability of your code. That is not so say that you should not use <code>pandas</code>, but rather that you justify to yourself that <code>pandas</code> isn't making your life harder in lieu of using standard <code>python</code> tools.</p> <p>We appreciate that <code>pandas</code> is a gateway into <code>python</code> programming for many people, and for that reason it becomes habitual way of coding. However... code containing lots of <code>pandas</code> operations are almost impossible to review, and therefore have the capacity to accumulate vast numbers of bugs.</p> <ul> <li>In general, <code>pandas</code> makes column-wise operations and IO (reading/writing files) dead easy. That said, <code>pandas</code> column-wise operations are inherited from <code>numpy</code>, and <code>numpy</code> is generally accepted in the place of dataframes.</li> <li><code>pandas</code> is enormous, in many ways. If it can be omitted from your code then you can make big savings in terms of memory usage and requirements clashes, and even CPU time.</li> <li>Instead of Googling how to achieve something in <code>pandas</code> with an obscure chaining of functions, break the problem down and solve it yourself. It is highly unlikely that the <code>pandas</code> approach to reshaping your data will beat using tools from <code>numpy</code>, <code>itertools</code>, <code>functools</code> and <code>toolz</code>, even if you switch to representing data in <code>numpy</code> arrays or even as <code>list</code> of <code>dict</code> (<code>[{'value': 21}, {'value': 45}]</code>).</li> <li>If you would like guidance, tips or ideas on how to un<code>panda</code> your code then ask on the <code>dev</code> Slack Channel - we're all here to help!</li> </ul>"},{"location":"guidelines/#design-patterns","title":"Design patterns","text":"<p>Favour the following design patterns, in this order:</p> <ol> <li>Functional programming: using functions to do one thing well, not altering the original data.</li> <li>Modular code: using functions to eliminate duplication, improve readability and reduce indentation.</li> <li>Object-oriented programming: Generally avoid, unless you are customising an API (for example <code>DataFrame</code>) or defining your own API.</li> </ol> <p>If you are not, at least, adhering to a modular style then you have gone very wrong. You should implement unit tests for each of your functions, something which is generally more tricky for object-oriented programming.</p>"},{"location":"guidelines/#programming","title":"Programming","text":""},{"location":"guidelines/#mandatory","title":"Mandatory","text":"<p>NB: eventually these checks will be automatic</p> <ul> <li>Don't compare boolean values to <code>True</code> or <code>False</code>.</li> <li>Favour <code>is not condition</code> over <code>not condition is</code></li> <li>Don't compare a value to <code>None</code> (<code>value == None</code>), always favour <code>value is None</code></li> </ul>"},{"location":"guidelines/#encouraged","title":"Encouraged","text":"<ul> <li>Favour <code>logging</code> over <code>print</code></li> <li>Favour using configuration files, or (faster/lazier/less good/ok) <code>GLOBAL_VARIABLES</code> near the top of your code, rather than repeated use of hard-coded variables in your code, particularly when with URL and file path variables like <code>s3://path/to/some/fixed/place</code>, but also for repeated numeric hyperparameters.</li> </ul>"},{"location":"guidelines/#critical-thinking","title":"Critical thinking","text":"<p>The following questions should be going through your mind when developing your code. If you don't quite understand the wording or intention of the following questions then we encourage you to ask in the <code>dev</code> Slack channel!</p> <ul> <li>Surely this simple problem already has an elegant solution?</li> <li>How many copies of the data am I making in memory?</li> <li>Where do my variables go out of scope?</li> <li>How can I avoid creating a new variable at all costs? (think iterator, scope, <code>lru_cache</code>)</li> <li>Have I made sure that I only run expensive or time consuming processes as few times as possible?</li> </ul>"},{"location":"guidelines/#naming-conventions","title":"Naming conventions","text":""},{"location":"guidelines/#mandatory_1","title":"Mandatory","text":"<ul> <li>Functions / methods: <code>function</code>, <code>my_function</code> (snake case)</li> <li>Variables / attributes: <code>variable</code>, <code>my_var</code> (snake case)</li> <li>Class: <code>Model</code>, <code>MyClass</code> (camel case)</li> <li>Module / file names / directory names: <code>module</code>, <code>file_name.py</code>, <code>dir_name</code> (camel case)</li> <li>Global* / constants: <code>A_GLOBAL_VARIABLE</code> (screaming snake case)</li> </ul> <p>* here we use \"Global\" to mean constants in scope at the module level, not the <code>global</code> level. Don't use <code>global</code>, ever.</p>"},{"location":"guidelines/#encouraged_1","title":"Encouraged","text":"<ul> <li>Keep all names as short and descriptive as possible. Variable names such as <code>x</code> or <code>df</code> are highly discouraged unless they are genuinely representing abstract concepts.</li> <li>Favour good naming conventions over helpful comments</li> </ul>"},{"location":"guidelines/#spaces-and-spacing","title":"Spaces and spacing","text":"<p>NB: that using the autoformatter black in your IDE will resolve almost all of the following</p>"},{"location":"guidelines/#encouraged_2","title":"Encouraged","text":"<ul> <li>Use the absolute minimum number of indents of your code. You can achieve this by writing modular code, and inverting logic, for example:</li> </ul> <pre><code>def something(args):\n    for item_collection in args:         # 1 tab\n        if item_collection.exists():     # 2 tabs\n            the_sum = 0                  # 3 tabs\n            for item in item_collection:\n                the_sum += item.value()  # 4 tabs\n            print(the_sum)\n</code></pre> <p>can become:</p> <pre><code>def sum_values(item_collection):\n    the_sum = sum(item.value() for item in item_collection)\n    print(the_sum)\n\ndef something(args):\n    for item_collection in args:         # 1 tab\n        if not item_collection.exists(): # 2 tabs\n            continue  # 3 tabs, buts unindents following lines\n        sum_values(item_collection)      # 2 tabs\n</code></pre> <ul> <li>Put a space before starting block comments <code># like this</code>, <code>#not this</code></li> <li>Inline comments need two spaces before them <code>a = 2 # like this</code></li> <li>Keep lines to 88 (officially 79) characters or less. You can achieve this by utilising other parts of this guideline, particularly with regards to creating modular code. Splitting over multiple lines is, of course, permissible so long as it doesn't conflict with legibility.</li> <li>When declaring default values, never put spaces around operators like <code>=</code>, i.e <code>def this_is_ok(param=1)</code>, <code>def this_is_NOT_ok(param = 1)</code>. Otherwise, all operators must always have a single space on either side.</li> <li>Separate function and class arguments with a comma and a space, i.e. <code>do_thing(1, b=2)</code> and not <code>do_thing(1,b=2)</code>.</li> </ul>"},{"location":"guidelines/#comments-and-docs","title":"Comments and docs","text":""},{"location":"guidelines/#mandatory_2","title":"Mandatory","text":"<ul> <li>At least a basic docstring is required for every function/method and class</li> <li>Full, explanatory docstrings are required for all function/methods and classes if it will be used in the main body of a code routine.</li> <li>Use Google-style: https://sphinxcontrib-napoleon.readthedocs.io/en/latest/example_google.html</li> <li>If you are using type hints then then you can write e.g. <code>my_arg (str): description</code> as simply <code>my_arg: description</code>.</li> </ul>"},{"location":"guidelines/#encouraged_3","title":"Encouraged","text":"<ul> <li>Don't state the obvious in comments</li> <li>Before writing a comment, consider whether that information would be better be encoded in a useful variable or function name.</li> </ul>"},{"location":"guidelines/#workflow","title":"Workflow","text":"<p>This builds on a much greater body of work, laid out in nestauk/github_support. For avoidance of doubt, branches must be linked to a GitHub issue and named accordingly:</p> <pre><code>{GitHub issue number}_{tinyLowerCamelDescription}\n</code></pre> <p>For example <code>14_readme</code>, which indicates that this branch refered to this issue.</p> <p>You should generally favour having a <code>dev</code> branch, in addition to your <code>main</code> (<code>master</code>) branch. Never commit to <code>dev</code>, <code>master</code> or <code>main</code>.</p> <p>Only pull requests from branches named <code>{GitHub issue number}_{tinyLowerCamelDescription}</code> should ever be accepted.</p> <p>Please make all PRs and issues reasonably small: they should be trying to achieve roughly one task. Inevitably some simple tasks spawn large numbers of utils, and sometimes these detract from the original PR. In this case, you should stack an new PR on top of your \"base\" PR, for example as <code>{GitHub issue number}_{differentLowerCamelDescription}</code>. In this case the PR / Git merging tree will look like:</p> <pre><code>dev &lt;-- 123_originalThing &lt;-- 423_differentThing &lt;-- 578_anotherDifferentThing\n</code></pre> <p>We can then merge the PR <code>123_originalThing</code> into <code>dev</code>, then <code>423_differentThing</code> into <code>dev</code> (after calling <code>git merge dev</code> on <code>423_differentThing</code>), etc until the chain is merged entirely. The nominated reviewer should review the entire chain, before the merge can go ahead. PRs should only be merged if all tests and a review has been signed off.</p>"},{"location":"quickstart/","title":"Project Set Up","text":""},{"location":"quickstart/#your-guide-to-setting-up-a-nesta-cookiecutter-project","title":"Your Guide to Setting Up a Nesta Cookiecutter Project","text":"<p>In this page you will learn how to set up a project using the cookiecutter. The steps are different depending on whether you are the first one setting up a project (Project Configurer) or whether a project already exists and you are just setting it up locally (Team Member).</p>"},{"location":"quickstart/#project-configurer","title":"Project Configurer","text":"<ol> <li> <p>Request repository setup: First things first, you need a repo created for you. Submit a Create a repo in the Nesta GitHub org issue from the github_support issue page. You will need to provide a project name, suggested repo name, whether public/private, github teams involved, team leading the project, short and long description of the project. An empty repo will be set up for you and you will receive a notification when this is done.</p> </li> <li> <p>Set up your project locally: It is important that you do not clone the repo yet! Instead, follow these steps:</p> </li> <li> <p>Open the terminal and <code>cd</code> to a folder where you eventually want your repo to be</p> </li> <li>Run <code>cookiecutter https://github.com/nestauk/ds-cookiecutter</code>. This will automatically install the latest version. If you want to install a different version run <code>cookiecutter https://github.com/nestauk/ds-cookiecutter -c &lt;VERSION TAG&gt;</code></li> <li> <p>You will be presented with the following:</p> <ul> <li><code>You've downloaded ~.cookiecutters/ds-cookiecutter before. Is it okay to delete and re-download it?[yes]</code> press Enter to confirm yes, it's always best to use the latest version.</li> <li><code>project_name [Project_name]</code>: add_a_name_here</li> <li><code>repo_name [add_a_name_here]</code>: add_a_name_here</li> <li><code>author_name [Nesta]</code>: add_author or press Enter to confirm Nesta</li> <li><code>description [A short description of the project.]</code>: add short description</li> <li><code>Select openness: 1 - public 2 - private Choose from 1, 2 [1]</code>: regardless of the choice you can always change it in the future</li> </ul> </li> <li> <p><code>cd</code> to project directory and run <code>make install</code> to:</p> <ul> <li>Create a conda environment with a name corresponding to the repo_name prompt and install the project package and its dependencies</li> <li>Configure and install Git pre-commit hooks</li> </ul> </li> <li> <p>Connect your local project to github: You have set up your project locally and now you have to connect it to the remote repo. When you change directory to your created project folder, you will see that you are in a git repository and the generated cookiecutter has committed itself to the <code>0_setup_cookiecutter</code> branch. Connect to the git repo by running <code>git remote add origin git@github.com:nestauk/&lt;REPONAME&gt;</code> to point your local project to the configured repository.</p> </li> <li> <p>Merging your new branch: You are on <code>0_setup_cookiecutter</code>, whist <code>dev</code> is empty. They have diverging histories so you won't be able to push any work to <code>dev</code>. For this reason you need to merge <code>0_setup_cookiecutter</code> to <code>dev</code> by running:</p> </li> </ol> <pre><code>            git checkout 0_setup_cookiecutter\n            git branch dev 0_setup_cookiecutter -f\n            git checkout dev\n            git push origin dev -f\n</code></pre> <ol> <li>You are all set! You can delete the <code>0_setup_cookicutter</code> branch and enjoy coding!</li> </ol>"},{"location":"quickstart/#team-members","title":"Team Members","text":"<ul> <li>Open the terminal and <code>cd</code> into a folder where you want the project set up.</li> <li>Clone the repository by running <code>git clone &lt;REPONAME&gt;</code> and <code>cd</code> into the repository.</li> <li>Run <code>make install</code> to configure the development environment.</li> </ul>"},{"location":"roadmap/","title":"Roadmap","text":""},{"location":"structure/","title":"Structure","text":"<p>This page lays out where things belong according to high-level concepts.</p> <p>A direct tree representation of the folder hierarchy is also available.</p> <p>Example structures will soon be available to help you structure the lower-level folders which the cookiecutter leaves to you.</p> <p>In the following sections I use <code>src/</code> to denote the project name to avoid awkward <code>&lt;project_name&gt;</code> placeholders.</p>"},{"location":"structure/#project-configuration-makefile","title":"Project configuration - <code>Makefile</code>","text":"<p>We use <code>make</code> to manage tasks relating to project setup/configuration/recurring tasks.</p> <p><code>make</code> is one of the simplest ways for managing steps that depend on each other, such as project configuration and is a common tool on Unix-based platforms.</p> <p>Running <code>make</code> from the project base directory will document the commands available along with a short description.</p> <pre><code>Available rules:\n\nclean               Delete all compiled Python files\nconda-update        Update the conda-environment based on changes to `environment.yaml`\nconda-remove        Remove the conda-environment cleanly\ndocs                Build the API documentation\ndocs-clean          Clean the built API documentation\ndocs-open           Open the docs in the browser\ninputs-pull         Pull `inputs/` from S3\ninstall             Install a project: create conda env; install local package; setup git hooks; setup metaflow+AWS\npip-install         Install our package and requirements in editable mode (including development dependencies)\n</code></pre> <p>Where appropriate these make commands will automatically be run in the conda environment for a project.</p>"},{"location":"structure/#git-hooks","title":"Git hooks","text":"<p>We use pre-commit to check the integrity of git commits before they happen.</p> <p>The steps are specified in <code>.pre-commit-config.yaml</code>.</p> <p>Currently the steps that are taken are:</p> <ul> <li>Run the black code autoformatter<ul> <li>This provides a consistent code style across a project and minimises messy git diffs (sometimes the code formatted by black may look \"uglier\" in places but this is the price we pay for having an industry standard with minimal cognitive burden)</li> </ul> </li> <li>Check that no large files were accidentally committed</li> <li>Check that there are no merge conflict strings (e.g. <code>&gt;&gt;&gt;&gt;&gt;</code>) lingering in files</li> <li>Fix the end of files to work across operating systems</li> <li>Trim trailing whitespace in files</li> <li>Check Toml files are well formed</li> <li>Check Yaml files are well formed</li> <li>Check we are no committing directly to <code>dev</code>, <code>master</code>, or <code>main</code></li> <li>Run the prettier formatter (covers files such as Markdown/JSON/YAML/HTML)</li> </ul> <p>Warning: You need to run <code>git commit</code> with your conda environment activated. This is because by default the packages used by pre-commit are installed into your project's conda environment. (note: <code>pre-commit install --install-hooks</code> will install the pre-commit hooks in the currently active environment).</p> <p>In time we will be integrating <code>flake8</code> into these pre-commit hooks. You can already lint your code using (a quite opinionated) flake8 configuration defined in <code>setup.cfg</code> by running <code>make lint</code>.</p>"},{"location":"structure/#reproducable-environment","title":"Reproducable environment","text":"<p>The first step in reproducing an analysis is always reproducing the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together.</p> <p>By listing all of your requirements in the repository you can easily track the packages needed to recreate the analysis, but what tool should we use to do that?</p> <p>Whilst popular for scientific computing and data-science, conda poses problems for collaboration and packaging:</p> <ul> <li>It is hard to reproduce a conda-environment across operating systems</li> <li>It is hard to make your environment \"pip-installable\" if your environment is fully specified by conda</li> </ul>"},{"location":"structure/#files","title":"Files","text":"<p>Due to these difficulties, we recommend only using conda to create a virtual environment and list dependencies not available through <code>pip install</code> (one prominent example of this is <code>graph-tool</code>).</p> <ul> <li> <p><code>environment.yaml</code> - Defines the base conda environment and any dependencies not \"pip-installable\".</p> </li> <li> <p><code>requirements.txt</code> - Defines the dependences required to run the code.</p> <p>If you need to add a dependency, chances are it goes here!</p> </li> <li> <p><code>requirements_dev.txt</code> - Defines development dependencies.</p> <p>These are for dependencies that are needed during development but not needed to run the core code. For example, packages to build documentation, run tests, and <code>ipykernel</code> to run code in <code>jupyter</code> (It's likely that you never need to think about this file)</p> </li> </ul>"},{"location":"structure/#commands","title":"Commands","text":"<ul> <li><code>make conda-update</code> - Update an existing conda environment (created by <code>make install</code>) from <code>environment.yaml</code> and run <code>make pip-install</code>.</li> <li><code>make conda-remove</code> - Remove an existing conda environment, tidying up the cookiecutters internal state.</li> <li><code>make pip-install</code> - Install our package and requirements in editable mode (including development dependencies).</li> </ul>"},{"location":"structure/#roadmap","title":"Roadmap","text":"<p>See roadmap for plans on improving packaging and reproducibility with Poetry and Docker.</p>"},{"location":"structure/#secrets-and-configuration-env-and-srcconfig","title":"Secrets and configuration - <code>.env.*</code> and <code>src/config/*</code>","text":"<p>You really don't want to leak your AWS secret key or Postgres username and password on Github. Enough said \u2014 see the Twelve Factor App principles on this point.</p>"},{"location":"structure/#store-your-secrets-in-a-special-file","title":"Store your secrets in a special file","text":"<p>Create a <code>.env</code> file in the project root folder. Thanks to the <code>.gitignore</code>, this file should never get committed into the version control repository.</p> <p>Here's an example:</p> <pre><code># example .env file\nDATABASE_URL=postgres://username:password@localhost:5432/dbname\nOTHER_VARIABLE=something\n</code></pre> <p>We also have <code>.envrc</code> which contains non-secret project configuration shared across users such as the bucket that our input data is stored in, or the metaflow profile to use.</p> <p><code>direnv</code> automatically loads <code>.envrc</code> (which itself loads <code>.env</code>) making our configuration available.</p>"},{"location":"structure/#store-data-science-configuration-in-srcconfig","title":"Store Data-science configuration in <code>src/config/</code>","text":"<p>Few things scupper colloborative analysis like hard-coding hyper-parameters parameters deep in the code-base.</p> <p><code>src/config/base.yaml</code> provides a place to document choices made.</p> <p>For example, if you were working on a fuzzy-matching the PATSTAT patent database to the Companies House database and wanted to only merge above a certain match score you may add a section to the configuration like the following,</p> <pre><code>patstat_companies_house:\nmatch_threshold: 90\n</code></pre> <p>and load that value into your code with,</p> <pre><code>from src import config\n\nconfig[\"patstat_companies_house\"][\"match_threshold\"]\n</code></pre> <p>This centralisation provides a clearer log of decisions and decreases the chance that a different match threshold gets incorrectly used somewhere else in the codebase.</p> <p>Aside - as well as avoiding hard-coding parameters into our code, we should never hard-code full file paths, e.g. <code>/home/alex/GIT/my_fantastic_data_project/outputs/data/foo.json</code>, this will never work on anything other than your machine.</p> <p>Instead use relative paths and make use of <code>src.PROJECT_DIR</code> which will return the path to your project's base directory. This means you could specify the above path as <code>f\"{src.PROJECT_DIR}/outputs/data/foo.json\"</code> and have it work on everyone's machine!</p>"},{"location":"structure/#roadmap_1","title":"Roadmap","text":"<p>See the roadmap for some ideas about approaches to configuration.</p>"},{"location":"structure/#data-inputsdata-outputsdata-outputscache","title":"Data - <code>inputs/data</code>, <code>outputs/data</code>, <code>outputs/.cache</code>","text":"<p>Firstly, don't version control data (inputs or outputs) in git, generally you should use s3 (directly or through metaflow) to manage your data.</p>"},{"location":"structure/#inputsdata","title":"<code>inputs/data</code>","text":"<p>Put any data dependencies of your project that your code doesn't fetch here (E.g. if someone emailed you a spreadsheet with the results of a randomised control trial).</p> <p>Don't ever edit this raw data, especially not manually, and especially not in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable.</p> <p>Ideally, you should store it in AWS S3.  If you set the <code>S3_INPUT_PATH</code> environment variable (e.g. in <code>.envrc</code>) then you can use <code>make inputs-pull</code> to pull data from the configured S3 bucket.</p>"},{"location":"structure/#outputscache","title":"<code>outputs/.cache/</code>","text":"<p>This folder is for ephemeral data and any pipeline/analysis step should be runnable following the deletion of this folder's contents.</p> <p>For example, this folder could be used as a file-level cache (careful about cache invalidation!); to download a file from the web before immediately reading, transforming, and saving as a clean file in <code>outputs/data</code>; or to temporary data when prototyping.</p>"},{"location":"structure/#outputsdata","title":"<code>outputs/data</code>","text":"<p>This folder should contain transformed/processed data that is to be used in the final analysis or is a data output of the final analysis.</p> <p>Try to order this folder logically. For example, you may want subfolders organised by dataset, sections of analysis, or some other hierarchy that better captures your project.</p>"},{"location":"structure/#fetchingloading-data-srcgetters","title":"Fetching/loading data - <code>src/getters</code>","text":"<p>This folder should contain modules and functions which load our data. Anywhere in the code base we need to load data we should do so by importing and calling a getter (except prototyping in notebooks).</p> <p>This means that peppering calls like <code>pd.read_csv(\"path/to/file\", sep=\"\\t\", ...)</code> throughout the codebase should be strictly avoided.</p> <p>Following this approach means:</p> <ul> <li>If the format of <code>path/to/file</code> changes then we only have to make the change in one place</li> <li>We avoid inconsistencies such as forgetting to read a column in as a <code>str</code> instead of an <code>int</code> and thus missing leading zeroes.</li> <li>If we want to see what data is available, we have a folder in the project to go to and we let the code speak for itself as much as possible - e.g. the following is a lot more informative than an inline call to <code>pd.read_csv</code> like we had above</li> </ul> <pre><code>    # File: getters/companies_house.py\n\"\"\"Data getters for the companies house data.\n\n    Data source: https://download.companieshouse.gov.uk/en_output.html\n    \"\"\"\n    import pandas as pd\n\n    def get_sector() -&gt; pd.DataFrame:\n\"\"\"Load Companies House sector labels.\n\n        Returns:\n            Sector information for ...\n        \"\"\"\n        return pd.read_csv(\"path/to/file\", sep=\"\\t\", dtype={\"sic_code\": str})\n</code></pre>"},{"location":"structure/#pipeline-components-srcpipeline","title":"Pipeline components - <code>src/pipeline</code>","text":"<p>This folder contains pipeline components. Put as much data-science as possible here.</p> <p>We recommend the use of metaflow to write these pipeline components.</p> <p>Using metaflow:</p> <ul> <li>Gives us lightweight version control of data and models</li> <li>Gives us easy access to AWS batch computing (including GPU machines)</li> <li>Makes it easy to take data-science code into production</li> </ul>"},{"location":"structure/#shared-utilities-srcutils","title":"Shared utilities - <code>src/utils</code>","text":"<p>This is a place to put utility functions needed across different parts of the codebase.</p> <p>For example, this could be functions shared across different pieces of analysis or different pipelines.</p>"},{"location":"structure/#analysis-srcanalysis","title":"Analysis - <code>src/analysis</code>","text":"<p>Functionality in this folder takes the pipeline components (possibly combining them) and generates the plots/statistics to feed into reports.</p> <p>It is easier to say when shomething shouldn't be in <code>analysis</code> than when something should: If one part in <code>analysis</code> depends on another, then that suggests that the thing in common is likely either a pipeline component or a shared utility (i.e. sections of <code>analysis</code> should be completely independent).</p> <p>It is important that plots are persisted to disk (in <code>outputs/figures</code>).</p>"},{"location":"structure/#notebooks","title":"Notebooks","text":"<p>Notebook packages like Jupyter notebook are effective tools for exploratory data analysis, fast prototyping, and communicating results; however, between prototyping and communicating results code should be factored out into proper python modules.</p>"},{"location":"structure/#where-does-the-humble-notebook-live","title":"Where does the humble notebook live?","text":"<p>Notebooks should be placed as close to the place where their functionality will eventually reside as possible.</p> <p>For example, if you are prototyping a \"sentence transformer\" pipeline then that pipeline component will likely end up somewhere like <code>pipeline/sentence_transformer/</code>, therefore you should place the notebooks for prototyping this features in <code>pipeline/sentence_transformer/notebooks/</code>.</p> <p>If you're just getting started with a project and don't have a clear sense of the separation between <code>analysis</code>, <code>pipeline</code>, and <code>getters</code> yet (or it's too premature to split functionality across multiple places) then a sensible place to start is <code>analysis/&lt;high-level-description&gt;/notebooks/</code>.</p>"},{"location":"structure/#version-control","title":"Version control","text":"<p>Since notebooks are challenging objects for source control (e.g., diffs of the <code>json</code> are often not human-readable and merging is a nightmare), we use jupytext to pair <code>.ipynb</code> files with a human-readable and git-diffable <code>.py</code> file.</p> <p>These paired <code>.py</code> files should be committed to git, <code>.ipynb</code> files are git-ignored.</p> <p>To ensure jupytext works correctly you should start <code>jupyter</code> (notebook/lab) from the base directory of the project so that <code>jupyter</code> detects the <code>jupytext</code> configuration that lives in <code>jupytext.toml</code>.</p>"},{"location":"structure/#refactoring","title":"Refactoring","text":"<p>Everybody likes to work differently. Some like to eagerly refactor, keeping as little in notebooks as possible (or even eschewing notebooks entirely); where as others prefer to keep everything in notebooks until the last minute.</p> <p>We do not require you to work one way or the other as long as by the time you submit a pull request (PR) for your feature everything is refactored into python modules.</p> <p>Having said this, we recommended you frequently refactor the good parts - you'll thank yourself later!</p> <p>A warning sign you've left it too late to refactor is if you've got duplicates of functions across the codebase rather than importing from a logical place - if it's a data preprocessing task, put it in the pipeline at <code>src/pipelines/&lt;descriptive name for task&gt;</code>; if it's useful utility code, refactor it to <code>src/utils/</code>; if it's loading data, refactor it to <code>src/getters</code>.</p>"},{"location":"structure/#tips","title":"Tips","text":"<p>Add the following to your notebook (or IPython REPL):</p> <pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <p>Now when you save code in a python module, the notebook will automatically load in the latest changes without you having to restart the kernel, re-import the module etc.</p> <p>When it comes to refactoring, open the python file Jupytext pairs to your notebook in your editor of choice - now your notebook code is easily-readable and in the same environment you use to write python modules.</p>"},{"location":"structure/#share-with-gists","title":"Share with gists","text":"<p>As the git filter above stops the sharing of outputs directly via. the git repository, another way is needed to share the outputs of quick and dirty analysis. We suggest using Gists, particularly the Gist it notebook extension which adds a button to your notebook that will instantly turn the notebook into a gist and upload it to your github (as public/private) as long. This requires <code>jupyter_nbextensions_configurator</code> and a github personal access token.</p>"},{"location":"structure/#dont-install-jupyterjupyterlab-in-your-environment-use-ipykernel","title":"Don't install <code>jupyter</code>/<code>jupyterlab</code> in your environment, use <code>ipykernel</code>","text":"<p>You should avoid <code>jupyter</code>/<code>jupyterlab</code> as a dependency in the project environment.</p> <p>Instead add <code>ipykernel</code> as a dependency. This is a lightweight dependency that allows <code>jupyter</code>/<code>jupyterlab</code> installed elsewhere (e.g. your main conda environment or system installation) to run the code in your project.</p> <p>Run <code>python -m ipykernel install --user --name=&lt;project environment name&gt;</code> from within your project environment to allow jupyter to use your project's virtual environment.</p> <p>The advantages of this are:</p> <ul> <li>You only have to configure <code>jupyter</code>/<code>jupyterlab</code> once</li> <li>You will save disk-space</li> <li>Faster install</li> <li>Colleagues using other editors don't have to install heavy dependencies they don't use (you wouldn't be happy if someone sent you code that depended on VScode/pycharm/spyder)</li> </ul> <p>Note: <code>ipykernel</code> is also listed in <code>requirements_dev.txt</code> so you do not need to add it.</p>"},{"location":"structure/#report-outputsreports","title":"Report - <code>outputs/reports</code>","text":"<p>We are currently evaluating how we report data-science work - both at the project-level and the feature-level.</p> <p>Minimally, you should write reports in markdown putting them in <code>outputs/reports</code> and referencing plots in <code>outputs/figures</code>.</p> <p>We are experimenting with a toolchain using pandoc to generate HTML and PDF (LaTeX) outputs from a single (pandoc flavoured) markdown file, including facilitating the trivial inclusion of interactive altair plots within HTML outputs.</p>"},{"location":"structure/#tree","title":"Tree","text":"<pre><code>\u251c\u2500\u2500 &lt;REPO NAME&gt;                      |  PYTHON PACKAGE\n\u2502   \u251c\u2500\u2500 __init__.py                  |\n\u2502   \u251c\u2500\u2500 analysis/                    |  Analysis\n\u2502   \u251c\u2500\u2500 config                       |  Configuration\n\u2502   \u2502   \u251c\u2500\u2500 logging.yaml             |    logging configuration\n\u2502   \u2502   \u251c\u2500\u2500 base.yaml                |    global configuration (e.g. for tracking hyper-parameters)\n\u2502   \u2502   \u2514\u2500\u2500 pipeline/                |    pipeline configuration files\n\u2502   \u251c\u2500\u2500 getters/                     |  Data getters\n\u2502   \u251c\u2500\u2500 pipeline/                    |  Pipeline components\n\u2502   \u2514\u2500\u2500 utils/                       |  Utilities\n\u251c\u2500\u2500 docs/                            |  DOCUMENTATION\n\u251c\u2500\u2500 environment.yaml                 |  CONDA ENVIRONMENT SPECIFICATION (optional component)\n\u251c\u2500\u2500 requirements.txt                 |  PYTHON DEPENDENCIES NEEDED TO RUN THE CODE\n\u251c\u2500\u2500 requirements_dev.txt             |  PYTHON DEV DEPENDENCIES (e.g. building docs/running tests)\n\u251c\u2500\u2500 inputs/                          |  INPUTS (should be immutable)\n\u251c\u2500\u2500 jupytext.toml                    |  JUPYTEXT CONFIGURATION\n\u251c\u2500\u2500 LICENSE                          |\n\u251c\u2500\u2500 outputs/                          |  OUTPUTS PRODUCED FROM THE PROJECT\n\u251c\u2500\u2500 Makefile                         |  TASKS TO COORDINATE PROJECT (`make` shows available commands)\n\u251c\u2500\u2500 README.md                        |\n\u251c\u2500\u2500 setup.py                         |  ALLOWS US TO PIP INSTALL src/\n\u251c\u2500\u2500 setup.cfg                        |  ADDITIONAL PROJECT CONFIGURATION, e.g. flake8\n\u251c\u2500\u2500 .pre-commit-config.yaml          |  DEFINES CHECKS THAT MUST PASS BEFORE git commit SUCCEEDS\n\u251c\u2500\u2500 .gitignore                       |  TELLS git WHAT FILES WE DON'T WANT TO COMMIT\n\u251c\u2500\u2500 .github/                         |  GITHUB CONFIGURATION\n\u251c\u2500\u2500 .env                             |  SECRETS (never commit to git!)\n\u251c\u2500\u2500 .envrc                           |  SHARED PROJECT CONFIGURATION VARIABLES\n\u251c\u2500\u2500 .cookiecutter                    |  COOKIECUTTER SETUP &amp; CONFIGURATION (user can safely ignore)\n</code></pre>"},{"location":"examples/","title":"Example project structures","text":"<ul> <li> Constructing an industrial taxonomy using business website descriptions </li> </ul>"},{"location":"examples/industrial_taxonomy/","title":"Constructing an industrial taxonomy using business website descriptions","text":"<p> Example under construction </p> <p>This example project is (loosely) based on work ongoing in <code>nestauk/industrial-taxonomy</code>, and the structure below is based on lessons learned in this project.</p> <p>The project is split into four high-level tasks (, , , ) which we walk through.</p> <p>Elements of  in particular have been simplified to keep the emphasis on the project structure rather than the project itself.</p> <p>You can skip ahead to the project tree  if you want a birds-eye view.</p>"},{"location":"examples/industrial_taxonomy/#matching-of-glass-to-companies-house","title":"Matching of Glass to Companies House","text":"<p>Method</p> <p>By fuzzy-matching data about UK business websites to Companies House based on company names we obtain a link between the text on business websites (describing businesses' activities) and the SIC codes (official industry codes) of that company.</p> <p>This work was performed in a separate project with the results stored and versioned in S3 by Metaflow. This can easily be retrieved using the metaflow client API.</p> <code>getters/inputs/{glass,companies_house.py,glass_house.py}</code> <p>Fetch all the data via. metaflow's client API.</p> <code>analysis/eda</code> <p>Exploratory data analysis of these data-sources</p>"},{"location":"examples/industrial_taxonomy/#sic-classifier","title":"SIC classifier","text":"<p>Method</p> <p>Using the matched \"glass-house\" dataset train a classifier to predict SIC codes (this is developed as a general industry classifier that is agnostic to the SIC taxonomy as it is used elsewhere in the project).</p> <p>We can then conduct a meta-analysis looking at SIC codes that are under-represented by the classifiers predictions on validation data when compared to their \"true\" label obtained from the \"glass-house\" matching.</p> <code>pipeline/industry_classifier/{log_reg_model.py,transformer_model.py}</code> <p>Two competing models (not specific to SIC taxonomy)</p> <code>config/pipeline/industry_classifier/sic/*.yaml</code> <p>Parameterisation of model flows.</p> What is the extra <code>sic</code> folder doing in the filepath? <p>There's an extra folder, <code>sic</code>, in the <code>config/</code> path that isn't present in the <code>pipeline/</code> path because this project uses the industry classifier models across different taxonomies.</p> <p>In this case, <code>sic</code> denotes the fact that we are applying it to the SIC taxonomy and gives us a namespace within the <code>config/</code> directory to isolate multiple uses of the same flow.</p> Whats with all the <code>config/pipeline/**/*.yaml</code>? <p>In this example structure, we have individual YAML files for each pipeline component.</p> <p>Whilst you are free to mimic this structure now, it is currently more convenient to nest the <code>config/pipeline/**</code> structure within <code>base.yaml</code> as it is easily importable as a python <code>dict</code> - <code>from src import config</code>.</p> <p>Furthermore, because metaflows are run from the command line and we want to parameterise them with YAML from <code>config/pipeline**</code>, each <code>flow.py</code> file currently needs an accompanying <code>run.py</code> file to: - Load and parse the YAML config needed for the pipeline - Form the pipeline arguments into a command to run the metaflow on the command line - Run the metaflow from within <code>run.py</code> using Python's <code>subprocess</code> library. - Update a config file with the successful metaflow run ID (so that getters know which version of the data to fetch)</p> <p>This is a lot of leg-work and increases the surface-area for bugs, you may be better off hard-coding values into shell-scripts or a Makefile.</p> <code>getters/outputs/sic_classifier.py</code> <p>Load trained model, giving access to predict functionality</p> Why separate inputs and outputs in <code>getters/</code>? <p>Separating inputs and outputs in <code>getters</code> is useful when reading the code - it allows us to differentiate between what is produced in this project and what we depend on from elsewhere.</p> <p>This is less useful when writing code - the import <code>from src.getters.inputs.glass import get_sector</code> is very long. To provide a shorter import we can do the following:</p> <pre><code># File: src/getters/__init__.py\nfrom .inputs import glass\n\n# File: src/analysis/example.py\nfrom src.getters.glass import get_sector\n</code></pre> <p>Your directory structure doesn't always have to reflect the user-API!</p> Avoid <code>import</code>ing from <code>src/pipeline/</code> <p>If you find yourself importing functions from <code>src/pipeline</code> in <code>src/analysis</code> then that functionality likely belongs in <code>src/utils</code>.</p> <p>Furthermore, <code>src/analysis</code> should get results of <code>src/pipeline</code> via. functions in <code>src/getters</code>.</p> <p>One exception that may occasionally arise when working with metaflow is needing to import a flow object itself - e.g. to access a static method or class method it defines (metaflow doesn't permit storing functions as data artifacts).</p> <code>analysis/sic_classifier/</code> <p>Analysis of industry classifier models applied to SIC taxonomy</p> <ul> <li><code>model_selection.py</code> - Evaluate competing models and pick best</li> <li><code>sic_meta_analysis.py</code> - Meta-analysis looking at SIC codes under-represented in predictions and for which the model is over/under confident (informs which parts of the SIC taxonomy could be improved)</li> </ul>"},{"location":"examples/industrial_taxonomy/#hierarchical-topic-modelling","title":"Hierarchical topic modelling","text":"<p>Method</p> <p>By training a TopSBM hierarchical topic model on the business website descriptions we can use the topics and clusters generated by the model and combine them with the SIC code labels to generate measures of sector similarity (how similar are any two SIC codes based on their cluster membership probabilities) and sector homogeneity (how homogeneous are is the topic distribution of descriptions aggregated by SIC code).</p>"},{"location":"examples/industrial_taxonomy/#pre-processing","title":"Pre-processing","text":"<p>The first step is to process raw business website descriptions into clean, tokenised, n-grammed representation that can be passed to the topic model (this is re-used in ).</p> <code>pipeline/glass_description_ngrams/{flow,utils}.py</code> <p>Metaflow to run spacy entity recognition; convert to tokens; and generate n-grams based on co-occurrence</p> <p><code>flow.py</code> and <code>utils.py</code></p> <p>We have lots of <code>flow.py</code> and <code>utils.py</code>. Some might see this as bad because it's not super-informative; however as long as the parent folder has an informative name then it's good enough.</p> <code>config/glass_description_ngrams.yaml</code> <p>Parameterisation of the above metaflow.</p> <code>getters/outputs/glass_house.py</code> <p>Getter to fetch tokenised n-grams of business website descriptions.</p> <code>pipeline/glass_description_ngrams/notebooks/</code> <p>Sanity-checking of output results. Not a part of <code>analysis/</code> because not directly analysing/presenting these results.</p>"},{"location":"examples/industrial_taxonomy/#modelling","title":"Modelling","text":"<p>Now the topic model itself can be run.</p> <code>config/pipeline/topsbm.yaml</code> <p>No corresponding flow in pipeline! Imported from a different library (e.g. <code>ds-utils</code>) and used here</p> <code>getters/outputs/topsbm.py</code> <p>Fetch fitted model instance containing our inferred topics and clusters</p> <code>analysis/topsbm/</code> <ul> <li><code>model_metadata.py</code> - Output summary table of model fit, other metadata such as topic hierarchy, top words etc. - <code>sector_similarity.py</code> - Pair-wise similarity of SIC codes calculated using topsbm model outputs - <code>sector_homogeneity.py</code> - Homogeneity of SIC codes calculated using topsbm model outputs - <code>utils.py</code> -</li> </ul> Hang on... Why is <code>section_*.py</code> not a pipeline component? <p>It would be equally valid to place these in <code>pipeline/</code> (because they are computing transformations of data) but <code>analysis/</code> is also fine (and possibly better) because:</p> <ul> <li>Nothing else depends on these (so we aren't forced to refactor outside <code>analysis/</code>)</li> <li>The transformations done by these scripts are relatively quick (therefore there's no need to refactor into a metaflow in <code>pipeline/</code> to save others from having to recompute a long-running analysis)</li> <li>These scripts would need to exist anyway to visualise and summarise the results for reporting</li> </ul> Hang on... doesn't <code>utils.py</code> imply shared functionality in <code>analysis</code>? <p>If we had a flatter <code>analysis/</code> folder - e.g. everything in <code>analysis/topsbm/</code> was moved into <code>analysis/</code> - this would be unacceptable; however it's just about okay to have a short utils file here.</p> <p>If we weren't happy with this we could put it in <code>utils/topsbm.py</code>: - For: other pieces of analysis or pipeline components may need to use these functions in the future, now they can without refactoring - Against: in this case it's only one common function which we're pretty sure is only needed here and now lives further away from where it's used</p>"},{"location":"examples/industrial_taxonomy/#build-a-data-driven-taxonomy","title":"Build a data-driven taxonomy","text":"<p>Method</p> <ul> <li>Identify relevant terms within business website descriptions</li> <li>Build a co-occurrence network of terms and prune network</li> <li>Decompose co-occurrence into communities</li> <li>Label companies with their communities to add a new level onto the existing SIC taxonomy</li> <li>Apply industry classifier to perform similar meta-analysis to that done in </li> </ul>"},{"location":"examples/industrial_taxonomy/#keyword-extraction","title":"Keyword extraction","text":"<p>Use keyword extraction methods to tag business descriptions with items from the UNSPSC (a products and services taxonomy). Given this is successful, co-occurrence networks of products and services can be used to build a taxonomy. If this is unsuccessful, fall back on the n-gramming pipeline produced in .</p> <code>inputs/data/UNSPSC_English_v230701.xlsx</code> <p>New dataset for this project provided by the supplier as an excel spreadsheet</p> <code>analysis/eda/unspsc.py</code> <p>Explore the UNSPSC dataset</p> <code>getters/inputs/unspsc.py</code> <p>Function to load UNSPSC data</p> <ul> <li>Note: It's structured and clean enough that we don't need to do any preprocessing on it</li> </ul> <code>pipeline/keyword_extraction/*.py</code> <p>Metaflows to extract keywords from text using various methods and filter based on prescence in UNSPSC</p> <code>config/pipeline/keyword_extraction/*.yaml</code> <p>Parameterise above pipelines</p> <code>analysis/keyword_extraction/notebooks/</code> <p>None of the keyword extraction approaches worked out</p> <p>No need to refactor notebooks into script if materials are not produced for final reporting.</p> <p>Notebooks exploring results of keyword extraction methods and comparing their effectiveness.</p>"},{"location":"examples/industrial_taxonomy/#n-gramming-pipeline","title":"N-gramming pipeline","text":"<code>getters/outputs/glass_house.py</code> <p>Use same getter as produced in preprocessing step of .</p>"},{"location":"examples/industrial_taxonomy/#constructing-a-term-co-occurrence-network-and-generating-communities","title":"Constructing a term co-occurrence network and generating communities","text":"<code>pipeline/kw_cooccurrence_taxonomy/*.py</code> <p>Metaflow and utils to construct a co-occurrence network of terms; decompose into communities; and label companies with their community labels</p> <code>config/pipeline/kw_cooccurrence_taxonomy.yaml</code> <p>Parameterise above flow</p> <code>analysis/kw_cooccurrence_taxonomy/visualise_structure.py</code> <p>Visualise the structure of the new taxonomy (and it's dependent communities)</p>"},{"location":"examples/industrial_taxonomy/#applying-the-industry-classifier-to-the-new-taxonomy-level","title":"Applying the industry classifier to the new taxonomy level","text":"<code>config/pipeline/industry_classifier/kw_cooccurrence_taxonomy.yaml</code> <p>Apply the industry classifier to our new taxonomies labels</p> <code>analysis/kw_cooccurrence_taxonomy/industry_classifier.py</code> <p>Perform a meta-analysis for our new taxonomy as was done in  for the SIC taxonomy</p>"},{"location":"examples/industrial_taxonomy/#project-tree","title":"Project tree","text":"folders onlyfolders and files <pre><code>\u251c\u2500\u2500 inputs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 data\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 analysis\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 eda\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 notebooks\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 keyword_extraction\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 kw_cooccurrence_taxonomy\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 notebooks\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 sic_classifier\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 notebooks\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 topsbm\n    \u251c\u2500\u2500 config\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 pipeline\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 industry_classifier\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 kw_cooccurrence_taxonomy\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 sic\n    \u251c\u2500\u2500 getters\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 inputs\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 outputs\n    \u251c\u2500\u2500 pipeline\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 glass_description_ngrams\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 industry_classifier\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 notebooks\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 keyword_extraction\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 notebooks\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 kw_cooccurrence_taxonomy\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 notebooks\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 sic_taxonomy_data\n    \u2514\u2500\u2500 utils\n        \u251c\u2500\u2500 altair\n        \u2514\u2500\u2500 metaflow\n</code></pre> <pre><code>TODO\n</code></pre>"}]}