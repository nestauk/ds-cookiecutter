{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Nesta Data Science Cookiecutter","text":"<p>A standard project structure for reproducible and collaborative data science projects @ Nesta.</p>"},{"location":"#high-level-aims","title":"High-level aims","text":"<ul> <li>Get going on a new project quickly (but not too quickly!)</li> <li>Nudge users to using a common structure and best-practices to:<ul> <li>Enable data scientists @ Nesta to work with each other</li> <li>Increase reliability of data science @ Nesta</li> <li>Make our projects more reproducible</li> <li>Increase the value of codebases, and accompanying documentation/reports to stakeholders</li> <li>Make code easier to understand</li> </ul> </li> <li>Conceptual/structural overlap with production systems allows data scientists to get code into production faster/easier</li> <li>Teams/projects empowered to build on foundation</li> </ul>"},{"location":"guidelines/","title":"Managing Your Python Environment","text":"<p>The cookiecutter comes with built-in support to manage a project specific python (<code>conda</code>) environment via the makefile.</p> <p>What is <code>conda</code>? Conda is an open-source, cross-platform package management system that makes setting up Python environments easy. It is specifically designed for data scientists and analysts.</p> <p>When you run <code>make install</code>, a <code>conda</code> environment will be created for you with a name that matches the repo name. To activate it, you can run:</p> <p><code>conda activate &lt;repo_name&gt;</code></p> <p>For more context on how the cookiecutter creates a conda environment, click here. For more information on Python environments (from our Python guidelines), click here.</p> <p>Your conda environment is an encompassing python environment for your project, you can install/uninstall packages as necessary, and these will only exist in the context of the environment of your project.</p> <p>To check which packages are installed in your active Conda environment, use the following command:</p> <p><code>conda list &lt;repo_name&gt;</code></p>"},{"location":"guidelines/#installing-packages-with-pip","title":"Installing packages with pip","text":"<p><code>pip</code> is the standard package manager for Python. It allows you to install and manage additional libraries and dependencies that are not distributed as part of the standard library.</p> <p>To install a package using pip, you can use the following command:</p> <p><code>pip install package_name</code></p>"},{"location":"guidelines/#the-role-of-a-requirementstxt","title":"The Role of a requirements.txt","text":"<p>A <code>requirements.txt</code> is a file commonly found in Python projects, and is generated for you as part of a cookiecutter project setup. It lists the names of all Python packages that a Python application depends on. Each line of the requirements.txt file normally contains a package name, and optionally, the version number.</p> <p>Here's an example requirements.txt:</p> <pre><code>numpy=1.18.1\npandas=1.0.1\nscikit-learn\n</code></pre> <p>If you have a <code>requirements.txt</code> file, you can install all required packages with the following command:</p> <p><code>pip install -r requirements.txt</code></p> <p>This will install the specific versions of all the packages listed in the <code>requirements.txt</code> file. This is useful for ensuring consistent environments across different systems, or when deploying a Python application.</p> <p>Python\u2019s Achilles\u2019 heel is package dependency conflicts, so it is preferable to ensure you include package versions in your requirements.txt, to ensure that what works for you locally, works for everyone else who needs to work on the project. Not including dependencies can lead to errors for others trying to run your code, or even different results being produced, as differing versions of packages can have different functionalities.</p>"},{"location":"quickstart/","title":"Project Set Up","text":""},{"location":"quickstart/#your-guide-to-setting-up-a-nesta-cookiecutter-project","title":"Your Guide to Setting Up a Nesta Cookiecutter Project","text":"<p>In this page you will learn how to set up a project using the cookiecutter. The steps are different depending on whether you are the first one setting up a project (Project Configurer) or whether a project already exists and you are just setting it up locally (Team Member).</p>"},{"location":"quickstart/#project-configurer","title":"Project Configurer","text":"<ol> <li> <p>Request repository setup: First things first, you need a repo created for you. Submit a Create a repo in the Nesta GitHub org issue from the github_support issue page. You will need to provide a project name, suggested repo name, whether public/private, github teams involved, team leading the project, short and long description of the project. An empty repo will be set up for you and you will receive a notification when this is done.</p> </li> <li> <p>Set up your project locally: It is important that you do not clone the repo yet! Instead, follow these steps:</p> </li> <li> <p>Open the terminal and <code>cd</code> to a folder where you eventually want your repo to be</p> </li> <li>Run <code>cookiecutter https://github.com/nestauk/ds-cookiecutter</code>. This will automatically install the latest version. If you want to install a different version run <code>cookiecutter https://github.com/nestauk/ds-cookiecutter -c &lt;VERSION TAG&gt;</code></li> <li> <p>You will be presented with the following:</p> <ul> <li><code>You've downloaded ~.cookiecutters/ds-cookiecutter before. Is it okay to delete and re-download it?[yes]</code> press Enter to confirm yes, it's always best to use the latest version.</li> <li><code>project_name [Project_name]</code>: add_a_name_here</li> <li><code>repo_name [add_a_name_here]</code>: add_a_name_here</li> <li><code>author_name [Nesta]</code>: add_author or press Enter to confirm Nesta</li> <li><code>description [A short description of the project.]</code>: add short description</li> <li><code>Select openness: 1 - public 2 - private Choose from 1, 2 [1]</code>: regardless of the choice you can always change it in the future</li> </ul> </li> <li> <p><code>cd</code> to project directory and run <code>make install</code> to:</p> <ul> <li>Create a conda environment with a name corresponding to the repo_name prompt and install the project package and its dependencies</li> <li>Configure and install Git pre-commit hooks</li> </ul> </li> <li> <p>Connect your local project to github: You have set up your project locally and now you have to connect it to the remote repo. When you change directory to your created project folder, you will see that you are in a git repository and the generated cookiecutter has committed itself to the <code>0_setup_cookiecutter</code> branch. Connect to the git repo by running <code>git remote add origin git@github.com:nestauk/&lt;REPONAME&gt;</code> to point your local project to the configured repository.</p> </li> <li> <p>Merging your new branch: You are on <code>0_setup_cookiecutter</code>, whist <code>dev</code> is empty. They have diverging histories so you won't be able to push any work to <code>dev</code>. For this reason you need to merge <code>0_setup_cookiecutter</code> to <code>dev</code> by running:</p> </li> </ol> <pre><code>            git checkout 0_setup_cookiecutter\n            git branch dev 0_setup_cookiecutter -f\n            git checkout dev\n            git push origin dev -f\n</code></pre> <ol> <li>You are all set! You can delete the <code>0_setup_cookicutter</code> branch and enjoy coding!</li> </ol>"},{"location":"quickstart/#team-members","title":"Team Members","text":"<ul> <li>Open the terminal and <code>cd</code> into a folder where you want the project set up.</li> <li>Clone the repository by running <code>git clone &lt;REPONAME&gt;</code> and <code>cd</code> into the repository.</li> <li>Run <code>make install</code> to configure the development environment.</li> </ul>"},{"location":"structure/","title":"Structure","text":"<p>This page gives a guide to where things belong within the cookiecutter structure.</p> <p>A direct tree representation of the folder hierarchy is also given at the bottom.</p> <p>Here are a couple of examples from projects:</p> <ul> <li> <p>AHL - Out of Home Analysis</p> </li> <li> <p>AFS - Birmingham Early Years Data</p> </li> <li> <p>ASF - Heat Pump Readiness</p> </li> <li> <p>DS - Green Jobs</p> </li> </ul> <p>Note: In the following sections we use <code>src/</code> to denote the project name to avoid awkward <code>&lt;project_name&gt;</code> placeholders.</p>"},{"location":"structure/#project-configuration-makefile","title":"Project configuration - <code>Makefile</code>","text":"<p>We use <code>make</code> to manage tasks relating to project setup/configuration/recurring tasks.</p> <p><code>make</code> is one of the simplest ways for managing steps that depend on each other, such as project configuration and is a common tool on Unix-based platforms.</p> <p>Running <code>make</code> from the project base directory will document the commands available along with a short description.</p> <p>You should run <code>make install</code> when you start working on a cookiecutter project. This will create a conda environment with the name of your project and configure git as well.</p> <p>For more info on the Makefile, see here.</p>"},{"location":"structure/#git-hooks","title":"Git hooks","text":"<p>We use pre-commit to check the integrity of git commits before they happen.</p> <p>The steps are specified in <code>.pre-commit-config.yaml</code>.</p> <p>Currently the steps that are taken are:</p> <ul> <li>Run the black code auto formatter<ul> <li>This means we can have a consistent code style across projects when we are collaborating with other members of the team.</li> </ul> </li> <li>Check that no large files were accidentally committed</li> <li>Check that there are no merge conflict strings (e.g. <code>&gt;&gt;&gt;&gt;&gt;</code>) lingering in files</li> <li>Fix the end of files to work across operating systems</li> <li>Trim trailing whitespace in files</li> <li>Check Toml files are well formed</li> <li>Check Yaml files are well formed</li> <li>Check we are no committing directly to <code>dev</code>, <code>master</code>, or <code>main</code></li> <li>Run the prettier formatter (covers files such as Markdown/JSON/YAML/HTML)</li> </ul> <p>Warning: You need to run <code>git commit</code> with your conda environment activated. This is because by default the packages used by pre-commit are installed into your project's conda environment. (note: <code>pre-commit install --install-hooks</code> will install the pre-commit hooks in the currently active environment).</p>"},{"location":"structure/#reproducable-environment","title":"Reproducable environment","text":"<p>The first step in reproducing someone else\u2019s analysis is to reproduce the computational environment it was run in. You need the same tools, the same libraries, and the same versions to make everything play nicely together.</p> <p>By listing all of your requirements in the repository you can easily track the packages needed to recreate the analysis.</p> <p>Whilst popular for scientific computing and data science, conda poses problems for collaboration and packaging:</p> <ul> <li>It is hard to reproduce a conda-environment across operating systems</li> <li>It is hard to make your environment \"pip-installable\" if your environment is fully specified by conda</li> </ul>"},{"location":"structure/#files","title":"Files","text":"<p>Due to these difficulties, we recommend only using conda to create a virtual environment and list dependencies not available through <code>pip install</code> (one example of this is <code>graph-tool</code>).</p> <ul> <li> <p><code>environment.yaml</code> - Defines the base conda environment and any dependencies not \"pip-installable\".</p> </li> <li> <p><code>requirements.txt</code> - Defines the dependencies required to run the code.</p> <p>If you need to add a dependency, chances are it goes here!</p> </li> <li> <p><code>requirements_dev.txt</code> - Defines development dependencies.</p> <p>These are for dependencies that are needed during development but not needed to run the core code. For example, packages to build documentation, run tests, and <code>ipykernel</code> to run code in <code>jupyter</code> (It's likely that you never need to think about this file)</p> </li> </ul>"},{"location":"structure/#commands","title":"Commands","text":"<ul> <li><code>make conda-update</code> - Update an existing conda environment (created by <code>make install</code>) from <code>environment.yaml</code> and run <code>make pip-install</code>.</li> <li><code>make conda-remove</code> - Remove an existing conda environment, tidying up the cookiecutters internal state.</li> <li><code>make pip-install</code> - Install our package and requirements in editable mode (including development dependencies).</li> </ul>"},{"location":"structure/#secrets-and-configuration-env-and-srcconfig","title":"Secrets and configuration - <code>.env.*</code> and <code>src/config/*</code>","text":"<p>You really don't want to leak your AWS secret key or database username and password on Github. To avoid this you can:</p>"},{"location":"structure/#store-your-secrets-in-a-special-file","title":"Store your secrets in a special file","text":"<p>Create a <code>.env</code> file in the project root folder. Thanks to the <code>.gitignore</code>, this file should never get committed into the version control repository.</p> <p>Here's an example:</p> <pre><code># example .env file\nDATABASE_URL=postgres://username:password@localhost:5432/dbname\nOTHER_VARIABLE=something\n</code></pre> <p>We also have <code>.envrc</code> which contains non-secret project configuration shared across users such as the bucket that our input data is stored in.</p> <p><code>direnv</code> automatically loads <code>.envrc</code> (which itself loads <code>.env</code>) making our configuration available.</p>"},{"location":"structure/#store-data-science-configuration-in-srcconfig","title":"Store Data science configuration in <code>src/config/</code>","text":"<p>If there are certain variables that are useful throughout a codebase, it is useful to store these in a single place rather than having to define them throughout the project.</p> <p><code>src/config/base.yaml</code> provides a place to document these global variables.</p> <p>For example, if you were working on a fuzzy-matching the PATSTAT patent database to the Companies House database and wanted to only merge above a certain match score you may add a section to the configuration like the following,</p> <pre><code>patstat_companies_house:\n    match_threshold: 90\n</code></pre> <p>and load that value into your code with,</p> <pre><code>from src import config\n\nconfig[\"patstat_companies_house\"][\"match_threshold\"]\n</code></pre> <p>This centralisation provides a clearer log of decisions and decreases the chance that a different match threshold gets incorrectly used somewhere else in the codebase.</p> <p>Config files are also useful for storing model parameters.  Storing model parameters in a config makes it much easier to test different model configurations and document and reproduce your model once it\u2019s been trained. You can easily reference your config file to make changes and write your final documentation rather than having to dig through code. Depending on the complexity of your repository, it may make sense to create separate config files for each of your models. </p> <p>For example, if training an SVM classifier you may want to test different values of the regularisation parameter \u2018C\u2019. You could create a file called  <code>src/config/svm_classifier.yaml</code> to store the parameter values in the same way as before.</p> <p>Note - as well as avoiding hard-coding parameters into our code, we should never hard-code full file paths, e.g. <code>/home/Projects/my_fantastic_data_project/outputs/data/foo.json</code>, as this will never work on anything other than your machine.</p> <p>Instead use relative paths and make use of <code>src.PROJECT_DIR</code> which will return the path to your project's base directory. This means you could specify the above path as <code>f\"{src.PROJECT_DIR}/outputs/data/foo.json\"</code> and have it work on everyone's machine!</p>"},{"location":"structure/#data-inputsdata-outputsdata-outputscache","title":"Data - <code>inputs/data</code>, <code>outputs/data</code>, <code>outputs/.cache</code>","text":"<p>Generally, don't version control data (inputs or outputs) in git, it is best to use s3 (directly or through metaflow) to manage your data.</p>"},{"location":"structure/#inputsdata","title":"<code>inputs/data</code>","text":"<p>Put any data dependencies of your project that your code doesn't fetch here (E.g. if someone emailed you a spreadsheet with the results of a randomised control trial).</p> <p>Don't ever edit this raw data, especially not manually or in Excel. Don't overwrite your raw data. Don't save multiple versions of the raw data. Treat the data (and its format) as immutable.</p> <p>Ideally, you should store it in AWS S3. You can then use the ds-utils package, which has a neat way of pulling in data into dataframe. Alternatively, if you set the <code>S3_INPUT_PATH</code> environment variable (e.g. in <code>.envrc</code>) then you can use <code>make inputs-pull</code> to pull data from the configured S3 bucket.</p>"},{"location":"structure/#outputscache","title":"<code>outputs/.cache/</code>","text":"<p>This folder is for ephemeral data and any pipeline/analysis step should be runnable following the deletion of this folder's contents.</p> <p>For example, this folder could be used as a file-level cache (careful about cache invalidation!); to download a file from the web before immediately reading, transforming, and saving as a clean file in <code>outputs/data</code>; or to temporary data when prototyping.</p>"},{"location":"structure/#outputsdata","title":"<code>outputs/data</code>","text":"<p>This folder should contain transformed/processed data that is to be used in the final analysis or is a data output of the final analysis. Again, if this data is sensitive, it is always best to save on S3 instead!</p> <p>Try to order this folder logically. For example, you may want subfolders organised by dataset, sections of analysis, or some other hierarchy that better captures your project.</p>"},{"location":"structure/#fetchingloading-data-srcgetters","title":"Fetching/loading data - <code>src/getters</code>","text":"<p>This folder should contain modules and functions which load our data. Anywhere in the code base we need to load data we should do so by importing and calling a getter (except prototyping in notebooks).</p> <p>This means that lots of calls like <code>pd.read_csv(\"path/to/file\", sep=\"\\t\", ...)</code> throughout the codebase can be avoided.</p> <p>Following this approach means:</p> <ul> <li>If the format of <code>path/to/file</code> changes then we only have to make the change in one place</li> <li>We avoid inconsistencies such as forgetting to read a column in as a <code>str</code> instead of an <code>int</code> and thus missing leading zeros</li> <li>If we want to see what data is available, we have a folder in the project to go to and we let the code speak for itself as much as possible - e.g. the following is a lot more informative than an inline call to <code>pd.read_csv</code> like we had above</li> </ul> <p>Here are two examples: <pre><code>    # File: getters/companies_house.py\n    \"\"\"Data getters for the companies house data.\n\n    Data source: https://download.companieshouse.gov.uk/en_output.html\n    \"\"\"\n    import pandas as pd\n\n    def get_sector() -&gt; pd.DataFrame:\n        \"\"\"Load Companies House sector labels.\n\n        Returns:\n            Sector information for ...\n        \"\"\"\n        return pd.read_csv(\"path/to/file\", sep=\"\\t\", dtype={\"sic_code\": str})\n</code></pre> or using ds-utils: <pre><code>    #File: getters/asq_data.py\n    \"\"\"Data getters for the ASQ data.\n    \"\"\"\n    import pandas as pd\n    from nesta_ds_utils.loading_saving import S3\n\n    def get_asq_data() -&gt; pd.DataFrame:\n    \"\"\"Load ASQ data for assessments taken in 2022.\n\n\n    Returns: Dataframe of the ASQ data at individual level including information on \u2026\n    \"\"\"\n    return S3.download_obj(\n        bucket=\"bucket_name\",\n        path_from=\"data/raw/data_asq.csv\",\n        download_as=\"dataframe\",\n        kwargs_reading={\"engine\": \"python\"},\n    )\n</code></pre></p>"},{"location":"structure/#pipeline-components-srcpipeline","title":"Pipeline components - <code>src/pipeline</code>","text":"<p>This folder contains pipeline components. Put as much data science as possible here.</p> <p>We recommend the use of metaflow to write these pipeline components.</p> <p>Using metaflow:</p> <ul> <li>Gives us lightweight version control of data and models</li> <li>Gives us easy access to AWS batch computing (including GPU machines)</li> <li>Makes it easy to take data-science code into production</li> </ul>"},{"location":"structure/#shared-utilities-srcutils","title":"Shared utilities - <code>src/utils</code>","text":"<p>This is a place to put utility functions needed across different parts of the codebase.</p> <p>For example, this could be functions shared across different pieces of analysis or different pipelines.</p>"},{"location":"structure/#analysis-srcanalysis","title":"Analysis - <code>src/analysis</code>","text":"<p>Functionality in this folder takes the pipeline components (possibly combining them) and generates the plots/statistics to feed into reports.</p> <p>It is easier to say when shomething shouldn't be in <code>analysis</code> than when something should: If one part in <code>analysis</code> depends on another, then that suggests that the thing in common is likely either a pipeline component or a shared utility (i.e. sections of <code>analysis</code> should be completely independent).</p> <p>It is important that plots are saved in <code>outputs/</code> rather than in different areas of the repository.</p>"},{"location":"structure/#notebooks-srcnotebooks","title":"Notebooks - <code>src/notebooks</code>","text":"<p>Notebook packages like Jupyter notebook are effective tools for exploratory data analysis, fast prototyping, and communicating results; however, between prototyping and communicating results code should be factored out into proper python modules.</p> <p>We have a notebooks folder for all your notebook needs! For example, if you are prototyping a \"sentence transformer\" you can place the notebooks for prototyping this feature in notebooks, e.g. <code>notebooks/sentence_transformer/</code> or <code>notebooks/pipeline/sentence_transformer/</code>. </p> <p>Please try to keep all notebooks within this folder and primarily not on github, especially for code refactoring as the code will be elsewhere, e.g. in the pipeline. However, for collaborating, sharing and QA of analysis, you are welcome to push those to github. </p>"},{"location":"structure/#refactoring","title":"Refactoring","text":"<p>Everybody likes to work differently. Some like to eagerly refactor, keeping as little in notebooks as possible (or even eschewing notebooks entirely); whereas others prefer to keep everything in notebooks until the last minute.</p> <p>You are welcome to work in whatever way you\u2019d like, but try to always submit a pull request (PR) for your feature with everything refactored into python modules.</p> <p>We often find it easiest to refactor frequently,  otherwise you might get duplicates of functions across the codebase , e.g.  if it's a data preprocessing task, put it in the pipeline at <code>src/pipelines/&lt;descriptive name for task&gt;</code>; if it's useful utility code, refactor it to <code>src/utils/</code>; if it's loading data, refactor it to <code>src/getters</code>.</p>"},{"location":"structure/#tips","title":"Tips","text":"<p>Add the following to your notebook (or IPython REPL):</p> <pre><code>%load_ext autoreload\n%autoreload 2\n</code></pre> <p>Now when you save code in a python module, the notebook will automatically load in the latest changes without you having to restart the kernel, re-import the module etc.</p>"},{"location":"structure/#share-with-gists","title":"Share with gists","text":"<p>As the git filter above stops the sharing of outputs directly via. the git repository, another way is needed to share the outputs of quick and dirty analysis. We suggest using Gists, particularly the Gist it notebook extension which adds a button to your notebook that will instantly turn the notebook into a gist and upload it to your github (as public/private) as long. This requires <code>jupyter_nbextensions_configurator</code> and a github personal access token.</p>"},{"location":"structure/#dont-install-jupyterjupyterlab-in-your-environment-use-ipykernel","title":"Don't install <code>jupyter</code>/<code>jupyterlab</code> in your environment, use <code>ipykernel</code>","text":"<p>You should avoid <code>jupyter</code>/<code>jupyterlab</code> as a dependency in the project environment.</p> <p>Instead add <code>ipykernel</code> as a dependency. This is a lightweight dependency that allows <code>jupyter</code>/<code>jupyterlab</code> installed elsewhere (e.g. your main conda environment or system installation) to run the code in your project.</p> <p>Run <code>python -m ipykernel install --user --name=&lt;project environment name&gt;</code> from within your project environment to allow jupyter to use your project's virtual environment.</p> <p>The advantages of this are:</p> <ul> <li>You only have to configure <code>jupyter</code>/<code>jupyterlab</code> once</li> <li>You will save disk-space</li> <li>Faster install</li> <li>Colleagues using other editors don't have to install heavy dependencies they don't use</li> </ul> <p>Note: <code>ipykernel</code> is also listed in <code>requirements_dev.txt</code> so you do not need to add it.</p>"},{"location":"structure/#report-outputsreports","title":"Report - <code>outputs/reports</code>","text":"<p>You can write reports in markdown and put them in <code>outputs/reports</code> and reference plots in <code>outputs/figures</code>.</p>"},{"location":"structure/#tree","title":"Tree","text":"<pre><code>\u251c\u2500\u2500 &lt;REPO NAME&gt;                      |  PYTHON PACKAGE\n\u2502   \u251c\u2500\u2500 __init__.py                  |\n\u2502   \u251c\u2500\u2500 analysis/                    |  Analysis\n\u2502   \u251c\u2500\u2500 config                       |  Configuration\n\u2502   \u2502   \u251c\u2500\u2500 logging.yaml             |    logging configuration\n\u2502   \u2502   \u251c\u2500\u2500 base.yaml                |    global configuration (e.g. for tracking hyper-parameters)\n\u2502   \u2502   \u2514\u2500\u2500 pipeline/                |    pipeline configuration files\n\u2502   \u251c\u2500\u2500 getters/                     |  Data getters\n\u2502   \u251c\u2500\u2500 notebooks/                   |  Notebooks\n\u2502   \u251c\u2500\u2500 pipeline/                    |  Pipeline components\n\u2502   \u2514\u2500\u2500 utils/                       |  Utilities\n\u251c\u2500\u2500 docs/                            |  DOCUMENTATION\n\u251c\u2500\u2500 environment.yaml                 |  CONDA ENVIRONMENT SPECIFICATION (optional component)\n\u251c\u2500\u2500 requirements.txt                 |  PYTHON DEPENDENCIES NEEDED TO RUN THE CODE\n\u251c\u2500\u2500 requirements_dev.txt             |  PYTHON DEV DEPENDENCIES (e.g. building docs/running tests)\n\u251c\u2500\u2500 inputs/                          |  INPUTS (should be immutable)\n\u251c\u2500\u2500 jupytext.toml                    |  JUPYTEXT CONFIGURATION\n\u251c\u2500\u2500 LICENSE                          |\n\u251c\u2500\u2500 outputs/                          |  OUTPUTS PRODUCED FROM THE PROJECT\n\u251c\u2500\u2500 Makefile                         |  TASKS TO COORDINATE PROJECT (`make` shows available commands)\n\u251c\u2500\u2500 README.md                        |\n\u251c\u2500\u2500 setup.py                         |  ALLOWS US TO PIP INSTALL src/\n\u251c\u2500\u2500 setup.cfg                        |  ADDITIONAL PROJECT CONFIGURATION, e.g. flake8\n\u251c\u2500\u2500 .pre-commit-config.yaml          |  DEFINES CHECKS THAT MUST PASS BEFORE git commit SUCCEEDS\n\u251c\u2500\u2500 .gitignore                       |  TELLS git WHAT FILES WE DON'T WANT TO COMMIT\n\u251c\u2500\u2500 .github/                         |  GITHUB CONFIGURATION\n\u251c\u2500\u2500 .env                             |  SECRETS (never commit to git!)\n\u251c\u2500\u2500 .envrc                           |  SHARED PROJECT CONFIGURATION VARIABLES\n\u251c\u2500\u2500 .cookiecutter                    |  COOKIECUTTER SETUP &amp; CONFIGURATION (user can safely ignore)\n</code></pre>"},{"location":"structure/#the-makefile","title":"The Makefile","text":"<p>A Makefile is a build automation tool that is commonly used in software development projects. It is a text file that contains a set of rules and instructions for building, compiling, and managing the project. The primary role of a Makefile is to automate the build process and make it easier for developers to compile and run their code.</p> <p>Here are some key points to understand about the role of a Makefile in a codebase:</p> <ul> <li>Build Automation: A Makefile defines a set of rules that specify how to build the project. It includes instructions for compiling source code, linking libraries, and generating executable files or other artifacts. By using a Makefile, developers can automate the build process and ensure that all necessary steps are executed in the correct order.</li> <li>Dependency Management: Makefiles allow developers to define dependencies between different files or components of the project. This ensures that only the necessary parts of the code are rebuilt when changes are made, saving time and resources. Makefiles can track dependencies based on file timestamps or by explicitly specifying the relationships between files.</li> <li>Consistency and Reproducibility: With a Makefile, the build process becomes standardised and reproducible across different environments. Developers can share the Makefile with others, ensuring that everyone follows the same build steps and settings. This helps maintain consistency and reduces the chances of errors or inconsistencies in the build process.</li> <li>Customization and Extensibility: Makefiles are highly customizable and allow developers to define their own build targets and actions. This flexibility enables the integration of additional tools, such as code formatters, linters, or test runners, into the build process. Developers can easily extend the functionality of the Makefile to suit the specific needs of their project.</li> <li>Integration with Version Control: Makefiles are often included in the codebase and tracked by version control systems. This ensures that the build process is documented and can be easily reproduced by other team members. Makefiles can also be integrated into continuous integration (CI) pipelines, allowing for automated builds and tests whenever changes are pushed to the repository.</li> </ul> <p>As part of the cookiecutter, we have a Makefile that can perform some useful administrative tasks for us:</p> <pre><code>Available rules:\n\nclean               Delete all compiled Python files\nconda-update        Update the conda-environment based on changes to `environment.yaml`\nconda-remove        Remove the conda-environment cleanly\ndocs                Build the API documentation\ndocs-clean          Clean the built API documentation\ndocs-open           Open the docs in the browser\ninputs-pull         Pull `inputs/` from S3\ninstall             Install a project: create conda env; install local package; setup git hooks\npip-install         Install our package and requirements in editable mode (including development dependencies)\n</code></pre> <p>By far the most commonly used command is <code>make install</code>, so don't worry too much about the rest!</p>"},{"location":"examples/","title":"Example project structures","text":"<ul> <li> Constructing an industrial taxonomy using business website descriptions </li> </ul>"},{"location":"examples/industrial_taxonomy/","title":"Constructing an industrial taxonomy using business website descriptions","text":"<p> Example under construction </p> <p>This example project is (loosely) based on work ongoing in <code>nestauk/industrial-taxonomy</code>, and the structure below is based on lessons learned in this project.</p> <p>The project is split into four high-level tasks (, , , ) which we walk through.</p> <p>Elements of  in particular have been simplified to keep the emphasis on the project structure rather than the project itself.</p> <p>You can skip ahead to the project tree  if you want a birds-eye view.</p>"},{"location":"examples/industrial_taxonomy/#matching-of-glass-to-companies-house","title":"Matching of Glass to Companies House","text":"<p>Method</p> <p>By fuzzy-matching data about UK business websites to Companies House based on company names we obtain a link between the text on business websites (describing businesses' activities) and the SIC codes (official industry codes) of that company.</p> <p>This work was performed in a separate project with the results stored and versioned in S3 by Metaflow. This can easily be retrieved using the metaflow client API.</p> <code>getters/inputs/{glass,companies_house.py,glass_house.py}</code> <p>Fetch all the data via. metaflow's client API.</p> <code>analysis/eda</code> <p>Exploratory data analysis of these data-sources</p>"},{"location":"examples/industrial_taxonomy/#sic-classifier","title":"SIC classifier","text":"<p>Method</p> <p>Using the matched \"glass-house\" dataset train a classifier to predict SIC codes (this is developed as a general industry classifier that is agnostic to the SIC taxonomy as it is used elsewhere in the project).</p> <p>We can then conduct a meta-analysis looking at SIC codes that are under-represented by the classifiers predictions on validation data when compared to their \"true\" label obtained from the \"glass-house\" matching.</p> <code>pipeline/industry_classifier/{log_reg_model.py,transformer_model.py}</code> <p>Two competing models (not specific to SIC taxonomy)</p> <code>config/pipeline/industry_classifier/sic/*.yaml</code> <p>Parameterisation of model flows.</p> What is the extra <code>sic</code> folder doing in the filepath? <p>There's an extra folder, <code>sic</code>, in the <code>config/</code> path that isn't present in the <code>pipeline/</code> path because this project uses the industry classifier models across different taxonomies.</p> <p>In this case, <code>sic</code> denotes the fact that we are applying it to the SIC taxonomy and gives us a namespace within the <code>config/</code> directory to isolate multiple uses of the same flow.</p> Whats with all the <code>config/pipeline/**/*.yaml</code>? <p>In this example structure, we have individual YAML files for each pipeline component.</p> <p>Whilst you are free to mimic this structure now, it is currently more convenient to nest the <code>config/pipeline/**</code> structure within <code>base.yaml</code> as it is easily importable as a python <code>dict</code> - <code>from src import config</code>.</p> <p>Furthermore, because metaflows are run from the command line and we want to parameterise them with YAML from <code>config/pipeline**</code>, each <code>flow.py</code> file currently needs an accompanying <code>run.py</code> file to: - Load and parse the YAML config needed for the pipeline - Form the pipeline arguments into a command to run the metaflow on the command line - Run the metaflow from within <code>run.py</code> using Python's <code>subprocess</code> library. - Update a config file with the successful metaflow run ID (so that getters know which version of the data to fetch)</p> <p>This is a lot of leg-work and increases the surface-area for bugs, you may be better off hard-coding values into shell-scripts or a Makefile.</p> <code>getters/outputs/sic_classifier.py</code> <p>Load trained model, giving access to predict functionality</p> Why separate inputs and outputs in <code>getters/</code>? <p>Separating inputs and outputs in <code>getters</code> is useful when reading the code - it allows us to differentiate between what is produced in this project and what we depend on from elsewhere.</p> <p>This is less useful when writing code - the import <code>from src.getters.inputs.glass import get_sector</code> is very long. To provide a shorter import we can do the following:</p> <pre><code># File: src/getters/__init__.py\nfrom .inputs import glass\n\n# File: src/analysis/example.py\nfrom src.getters.glass import get_sector\n</code></pre> <p>Your directory structure doesn't always have to reflect the user-API!</p> Avoid <code>import</code>ing from <code>src/pipeline/</code> <p>If you find yourself importing functions from <code>src/pipeline</code> in <code>src/analysis</code> then that functionality likely belongs in <code>src/utils</code>.</p> <p>Furthermore, <code>src/analysis</code> should get results of <code>src/pipeline</code> via. functions in <code>src/getters</code>.</p> <p>One exception that may occasionally arise when working with metaflow is needing to import a flow object itself - e.g. to access a static method or class method it defines (metaflow doesn't permit storing functions as data artifacts).</p> <code>analysis/sic_classifier/</code> <p>Analysis of industry classifier models applied to SIC taxonomy</p> <ul> <li><code>model_selection.py</code> - Evaluate competing models and pick best</li> <li><code>sic_meta_analysis.py</code> - Meta-analysis looking at SIC codes under-represented in predictions and for which the model is over/under confident (informs which parts of the SIC taxonomy could be improved)</li> </ul>"},{"location":"examples/industrial_taxonomy/#hierarchical-topic-modelling","title":"Hierarchical topic modelling","text":"<p>Method</p> <p>By training a TopSBM hierarchical topic model on the business website descriptions we can use the topics and clusters generated by the model and combine them with the SIC code labels to generate measures of sector similarity (how similar are any two SIC codes based on their cluster membership probabilities) and sector homogeneity (how homogeneous are is the topic distribution of descriptions aggregated by SIC code).</p>"},{"location":"examples/industrial_taxonomy/#pre-processing","title":"Pre-processing","text":"<p>The first step is to process raw business website descriptions into clean, tokenised, n-grammed representation that can be passed to the topic model (this is re-used in ).</p> <code>pipeline/glass_description_ngrams/{flow,utils}.py</code> <p>Metaflow to run spacy entity recognition; convert to tokens; and generate n-grams based on co-occurrence</p> <p><code>flow.py</code> and <code>utils.py</code></p> <p>We have lots of <code>flow.py</code> and <code>utils.py</code>. Some might see this as bad because it's not super-informative; however as long as the parent folder has an informative name then it's good enough.</p> <code>config/glass_description_ngrams.yaml</code> <p>Parameterisation of the above metaflow.</p> <code>getters/outputs/glass_house.py</code> <p>Getter to fetch tokenised n-grams of business website descriptions.</p> <code>pipeline/glass_description_ngrams/notebooks/</code> <p>Sanity-checking of output results. Not a part of <code>analysis/</code> because not directly analysing/presenting these results.</p>"},{"location":"examples/industrial_taxonomy/#modelling","title":"Modelling","text":"<p>Now the topic model itself can be run.</p> <code>config/pipeline/topsbm.yaml</code> <p>No corresponding flow in pipeline! Imported from a different library (e.g. <code>ds-utils</code>) and used here</p> <code>getters/outputs/topsbm.py</code> <p>Fetch fitted model instance containing our inferred topics and clusters</p> <code>analysis/topsbm/</code> <ul> <li><code>model_metadata.py</code> - Output summary table of model fit, other metadata such as topic hierarchy, top words etc. - <code>sector_similarity.py</code> - Pair-wise similarity of SIC codes calculated using topsbm model outputs - <code>sector_homogeneity.py</code> - Homogeneity of SIC codes calculated using topsbm model outputs - <code>utils.py</code> -</li> </ul> Hang on... Why is <code>section_*.py</code> not a pipeline component? <p>It would be equally valid to place these in <code>pipeline/</code> (because they are computing transformations of data) but <code>analysis/</code> is also fine (and possibly better) because:</p> <ul> <li>Nothing else depends on these (so we aren't forced to refactor outside <code>analysis/</code>)</li> <li>The transformations done by these scripts are relatively quick (therefore there's no need to refactor into a metaflow in <code>pipeline/</code> to save others from having to recompute a long-running analysis)</li> <li>These scripts would need to exist anyway to visualise and summarise the results for reporting</li> </ul> Hang on... doesn't <code>utils.py</code> imply shared functionality in <code>analysis</code>? <p>If we had a flatter <code>analysis/</code> folder - e.g. everything in <code>analysis/topsbm/</code> was moved into <code>analysis/</code> - this would be unacceptable; however it's just about okay to have a short utils file here.</p> <p>If we weren't happy with this we could put it in <code>utils/topsbm.py</code>: - For: other pieces of analysis or pipeline components may need to use these functions in the future, now they can without refactoring - Against: in this case it's only one common function which we're pretty sure is only needed here and now lives further away from where it's used</p>"},{"location":"examples/industrial_taxonomy/#build-a-data-driven-taxonomy","title":"Build a data-driven taxonomy","text":"<p>Method</p> <ul> <li>Identify relevant terms within business website descriptions</li> <li>Build a co-occurrence network of terms and prune network</li> <li>Decompose co-occurrence into communities</li> <li>Label companies with their communities to add a new level onto the existing SIC taxonomy</li> <li>Apply industry classifier to perform similar meta-analysis to that done in </li> </ul>"},{"location":"examples/industrial_taxonomy/#keyword-extraction","title":"Keyword extraction","text":"<p>Use keyword extraction methods to tag business descriptions with items from the UNSPSC (a products and services taxonomy). Given this is successful, co-occurrence networks of products and services can be used to build a taxonomy. If this is unsuccessful, fall back on the n-gramming pipeline produced in .</p> <code>inputs/data/UNSPSC_English_v230701.xlsx</code> <p>New dataset for this project provided by the supplier as an excel spreadsheet</p> <code>analysis/eda/unspsc.py</code> <p>Explore the UNSPSC dataset</p> <code>getters/inputs/unspsc.py</code> <p>Function to load UNSPSC data</p> <ul> <li>Note: It's structured and clean enough that we don't need to do any preprocessing on it</li> </ul> <code>pipeline/keyword_extraction/*.py</code> <p>Metaflows to extract keywords from text using various methods and filter based on prescence in UNSPSC</p> <code>config/pipeline/keyword_extraction/*.yaml</code> <p>Parameterise above pipelines</p> <code>analysis/keyword_extraction/notebooks/</code> <p>None of the keyword extraction approaches worked out</p> <p>No need to refactor notebooks into script if materials are not produced for final reporting.</p> <p>Notebooks exploring results of keyword extraction methods and comparing their effectiveness.</p>"},{"location":"examples/industrial_taxonomy/#n-gramming-pipeline","title":"N-gramming pipeline","text":"<code>getters/outputs/glass_house.py</code> <p>Use same getter as produced in preprocessing step of .</p>"},{"location":"examples/industrial_taxonomy/#constructing-a-term-co-occurrence-network-and-generating-communities","title":"Constructing a term co-occurrence network and generating communities","text":"<code>pipeline/kw_cooccurrence_taxonomy/*.py</code> <p>Metaflow and utils to construct a co-occurrence network of terms; decompose into communities; and label companies with their community labels</p> <code>config/pipeline/kw_cooccurrence_taxonomy.yaml</code> <p>Parameterise above flow</p> <code>analysis/kw_cooccurrence_taxonomy/visualise_structure.py</code> <p>Visualise the structure of the new taxonomy (and it's dependent communities)</p>"},{"location":"examples/industrial_taxonomy/#applying-the-industry-classifier-to-the-new-taxonomy-level","title":"Applying the industry classifier to the new taxonomy level","text":"<code>config/pipeline/industry_classifier/kw_cooccurrence_taxonomy.yaml</code> <p>Apply the industry classifier to our new taxonomies labels</p> <code>analysis/kw_cooccurrence_taxonomy/industry_classifier.py</code> <p>Perform a meta-analysis for our new taxonomy as was done in  for the SIC taxonomy</p>"},{"location":"examples/industrial_taxonomy/#project-tree","title":"Project tree","text":"folders onlyfolders and files <pre><code>\u251c\u2500\u2500 inputs\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 data\n\u2514\u2500\u2500 src\n    \u251c\u2500\u2500 analysis\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 eda\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 notebooks\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 keyword_extraction\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 kw_cooccurrence_taxonomy\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 notebooks\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 sic_classifier\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 notebooks\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 topsbm\n    \u251c\u2500\u2500 config\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 pipeline\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 industry_classifier\n    \u2502\u00a0\u00a0         \u251c\u2500\u2500 kw_cooccurrence_taxonomy\n    \u2502\u00a0\u00a0         \u2514\u2500\u2500 sic\n    \u251c\u2500\u2500 getters\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 inputs\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 outputs\n    \u251c\u2500\u2500 pipeline\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 glass_description_ngrams\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 industry_classifier\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 notebooks\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 keyword_extraction\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 notebooks\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 kw_cooccurrence_taxonomy\n    \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 notebooks\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 sic_taxonomy_data\n    \u2514\u2500\u2500 utils\n        \u251c\u2500\u2500 altair\n        \u2514\u2500\u2500 metaflow\n</code></pre> <pre><code>TODO\n</code></pre>"}]}